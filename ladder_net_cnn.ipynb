{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ladder_net_cnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"woHpwj0ArpTy","colab_type":"text"},"cell_type":"markdown","source":["# Ladder Network with Convolutional Layers\n","**Input**: M,N,K specifying the general architecture: INPUT -> [[CONV -> RELU]*N -> POOL]*M -> [FC -> RELU]*K -> FC\n","\n","The Ladder network typically has a following layer structure: [Input size, say 700, 1000, 500, 250, 250, 250, 10]"]},{"metadata":{"id":"ix-ac8gfsbiY","colab_type":"text"},"cell_type":"markdown","source":["# Data"]},{"metadata":{"id":"1A_tCHSiroc0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"fceb7cbb-f33a-4ee5-c9aa-612c0d5c990a","executionInfo":{"status":"ok","timestamp":1541367631660,"user_tz":300,"elapsed":7484,"user":{"displayName":"julian buchel","photoUrl":"","userId":"03709653353653158720"}}},"cell_type":"code","source":["!pip install attributedict\n","import numpy as np\n","from sklearn.decomposition import PCA\n","import scipy.io as sio\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","import os\n","import random\n","from random import shuffle\n","from skimage.transform import rotate\n","import scipy.ndimage\n","from sklearn.model_selection import train_test_split\n","import scipy\n","\n","def load_pavia():\n","  \n","  !pip install GoogleDriveDownloader\n","  from google_drive_downloader import GoogleDriveDownloader as gdd\n","  gdd.download_file_from_google_drive(file_id='146WN2eZ6Syf-z1KMVRw9GmZdBu_g1JBj',\n","                                    dest_path='./datasets/paviau.mat', unzip=False)\n","\n","  gdd.download_file_from_google_drive(file_id='1L9OoAHnLVmPGbfKx8NhEbugxMzE1PG4j',\n","                                    dest_path='./datasets/paviau_gt.mat', unzip=False)\n","\n","  X = sio.loadmat('./datasets/paviau.mat')['paviaU']\n","  y = sio.loadmat('./datasets/paviau_gt.mat')['paviaU_gt']\n","\n","  return X, y\n","  \n","  \n","def createPatches(X, y, windowSize=5, removeZeroLabels = True):\n","  margin = int((windowSize - 1) / 2)\n","  zeroPaddedX = padWithZeros(X, margin=margin)\n","  # split patches\n","  patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n","  patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n","  patchIndex = 0\n","  for r in range(margin, zeroPaddedX.shape[0] - margin):\n","      for c in range(margin, zeroPaddedX.shape[1] - margin):\n","          patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n","          patchesData[patchIndex, :, :, :] = patch\n","          patchesLabels[patchIndex] = y[r-margin, c-margin]\n","          patchIndex = patchIndex + 1\n","  if removeZeroLabels:\n","      patchesData = patchesData[patchesLabels>0,:,:,:]\n","      patchesLabels = patchesLabels[patchesLabels>0]\n","      patchesLabels -= 1\n","  return patchesData, patchesLabels\n","  \n","  \n","def padWithZeros(X, margin=2):\n","  newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n","  x_offset = margin\n","  y_offset = margin\n","  newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n","  return newX\n","  \n","def standartizeData(X):\n","  newX = np.reshape(X, (-1, X.shape[2]))\n","  scaler = preprocessing.StandardScaler().fit(newX)  \n","  newX = scaler.transform(newX)\n","  newX = np.reshape(newX, (X.shape[0],X.shape[1],X.shape[2]))\n","  return newX, scaler\n","  \n","  \n","def applyPCA(X, numComponents=75):\n","  newX = np.reshape(X, (-1, X.shape[2]))\n","  pca = PCA(n_components=numComponents, whiten=True)\n","  newX = pca.fit_transform(newX)\n","  newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n","  return newX, pca\n","  \n","  \n","def diff(first, second):\n","  second = set(second)\n","  return [item for item in first if item not in second]\n","\n","\n","def splitTrainTestSet(X, y, testRatio=0.10):\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=345,\n","                                                      stratify=y)\n","  return X_train, X_test, y_train, y_test\n","  \n","  \n","def AugmentData(X_train):\n","  for i in range(int(X_train.shape[0]/2)):\n","      patch = X_train[i,:,:,:]\n","      num = random.randint(0,2)\n","      if (num == 0):\n","          flipped_patch = np.flipud(patch)\n","      if (num == 1):\n","          flipped_patch = np.fliplr(patch)\n","      if (num == 2):\n","          no = random.randrange(-180,180,30)\n","          flipped_patch = scipy.ndimage.interpolation.rotate(patch, no,axes=(1, 0),\n","                                                             reshape=False, output=None, order=3, mode='constant', cval=0.0, prefilter=False)\n","\n","      patch2 = flipped_patch\n","      X_train[i,:,:,:] = patch2\n","\n","  return X_train\n","  \n","  \n","def oversampleWeakClasses(X, y):\n","  uniqueLabels, labelCounts = np.unique(y, return_counts=True)\n","  maxCount = np.max(labelCounts)\n","  labelInverseRatios = maxCount / labelCounts  \n","  # repeat for every label and concat\n","  newX = X[y == uniqueLabels[0], :, :, :].repeat(round(labelInverseRatios[0]), axis=0)\n","  newY = y[y == uniqueLabels[0]].repeat(round(labelInverseRatios[0]), axis=0)\n","  for label, labelInverseRatio in zip(uniqueLabels[1:], labelInverseRatios[1:]):\n","      cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n","      cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n","      newX = np.concatenate((newX, cX))\n","      newY = np.concatenate((newY, cY))\n","  np.random.seed(seed=42)\n","  rand_perm = np.random.permutation(newY.shape[0])\n","  newX = newX[rand_perm, :, :, :]\n","  newY = newY[rand_perm]\n","  return newX, newY\n","  \n","  \n","  \n","def savePreprocessedData(X_trainPatches, X_testPatches, y_trainPatches, y_testPatches, windowSize):\n","  \n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","  \n","  with open(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/XtrainWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, X_trainPatches)\n","  with open(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/XtestWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, X_testPatches)\n","  with open(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/ytrainWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, y_trainPatches)\n","  with open(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/ytestWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, y_testPatches)\n","      \n","      \n","      \n","      \n","# Global Variables\n","numComponents = 30\n","windowSize = 5\n","testRatio = 0.25\n","saved = True\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","if saved == False:\n","  X, y = load_pavia()\n","  X,_ = standartizeData(X)\n","  X,pca = applyPCA(X,numComponents=numComponents)\n","  XPatches, yPatches = createPatches(X, y, windowSize=windowSize)\n","  X_train, X_test, y_train, y_test = splitTrainTestSet(XPatches, yPatches, testRatio)\n","  print(X_train.shape)\n","  X_train, y_train = oversampleWeakClasses(X_train, y_train)\n","  X_train = AugmentData(X_train)\n","  savePreprocessedData(X_train, X_test, y_train, y_test, windowSize = windowSize)\n","  print(X_train.shape)\n","  \n","else:\n","  X_train = np.load(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/XtrainWindowSize\" + str(windowSize) + \".npy\")\n","  y_train = np.load(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/ytrainWindowSize\" + str(windowSize) + \".npy\")\n","  X_test = np.load(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/XtestWindowSize\" + str(windowSize) + \".npy\")\n","  y_test = np.load(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/ytestWindowSize\" + str(windowSize) + \".npy\")\n","  print(X_train.shape)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: attributedict in /usr/local/lib/python3.6/dist-packages (0.1.8)\n","Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from attributedict) (2.2.0)\n","Requirement already satisfied: tox>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from attributedict) (3.5.3)\n","Requirement already satisfied: easypackage>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from attributedict) (0.1.8)\n","Requirement already satisfied: deepdiff>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from attributedict) (3.3.0)\n","Requirement already satisfied: colour-runner>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from attributedict) (0.1.1)\n","Requirement already satisfied: setuptools>=30.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (40.5.0)\n","Requirement already satisfied: six<2,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (1.11.0)\n","Requirement already satisfied: py<2,>=1.4.17 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (1.7.0)\n","Requirement already satisfied: virtualenv>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (16.1.0)\n","Requirement already satisfied: pluggy<1,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (0.8.0)\n","Requirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (0.10.0)\n","Requirement already satisfied: filelock<4,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (3.0.9)\n","Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from deepdiff>=3.3.0->attributedict) (1.0)\n","Requirement already satisfied: blessings in /usr/local/lib/python3.6/dist-packages (from colour-runner>=0.0.5->attributedict) (1.7)\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","(128051, 5, 5, 30)\n"],"name":"stdout"}]},{"metadata":{"id":"IBRmrQQUy6D6","colab_type":"text"},"cell_type":"markdown","source":["# Conv. Ladder Net\n","Architecture: INPUT -> [[CONV -> RELU]*N -> POOL]*M -> [FC -> RELU]*K -> FC\n","\n","**Params**: N,M,K,filter_size (Array of length N)\n","\n","**Default**: N=3,M=0,K=1, filter_size=[3*PCA_comp=90, 30, 15]\n","\n","**For now use only convolution**\n"]},{"metadata":{"id":"FGFqQIyxsOYM","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","from attributedict.collections import AttributeDict\n","\n","def train(X,y,X_test=None,y_test=None,N=3,filter_size=[90,30,15],fc=[],kernel_size=5,\n","          denoising_cost=[10,1,0.1,0.1,0.1],num_epochs=150,batch_size=200,num_labeled=100,noise_std=0.3,lr=0.02,\n","          decay_after=15):\n","  \n","  assert len(denoising_cost) is 2+len(filter_size)+len(fc), \"Please specify denoising cost for every Layer. len(denoising_cost) != 2+len(fc)+len(filter_size)\"\n","  \n","  tf.reset_default_graph()\n","  tf.set_random_seed(12345)\n","  #We double the batch size here. This has the advantage that in case num_labeled is -1 (use all labels) we can use half of the\n","  #batch size for the clean encoder and the other half for the unsupervised run\n","  batch_size *= 2\n"," \n","  #Number of convolutions\n","  N = len(filter_size)\n","  #Number of fully connected layers\n","  K = len(fc)\n","  \n","  #Shape of X: (?,WND_SZE,WND_SZE, N_CHANNELS)\n","  WND_SZE = X.shape[1]\n","  N_CHANNELS = X.shape[3]\n","  N_CLASSES = len(np.unique(y))\n","  N_EXAMPLES = X.shape[0]\n","  DEPTH = X.shape[-1]\n","  \n","  L = K+N+2 #Input+Convs+Softmax\n","  \n","  #Create list of action,output-shape pairs, e.g. fs=[90,30,15] & fc=[100,50,20] would correspond to\n","  #{'conv',(?,5,5,90);'conv',(?,5,5,30);'conv',(?,5,5,15);'relu',(?,100);'relu',(?,50);'relu',(?,20)}\n","  #Implicit: 'flatten' & 'softmax'\n","  shapes = [('conv',s) for s in filter_size]+[('relu',s) for s in fc]+[('softmax',N_CLASSES)]\n","  \n","  num_labeled_tf = tf.placeholder(tf.int32, shape=())\n","  \n","  n_classes = len(np.unique(y_test))\n","  n_labeled_per_class = int(num_labeled/n_classes)\n","\n","  #Create X_labeled and X_unlabeled where X_labeled has num_labeled entries which are balanced w.r.t. the class labels\n","  indices = np.arange(len(y))\n","  i_labeled = []\n","  for c in range(n_classes):\n","        i = indices[y==c][:n_labeled_per_class]\n","        i_labeled += list(i)\n","\n","\n","  X_labeled = X[i_labeled,:,:,:]\n","  y_labeled = y[i_labeled]\n","\n","\n","  # If num_labeled == -1, we only create one dataset, else we create one labeled and unlabeled set and balance the batches\n","\n","  n_classes = len(np.unique(y_test))\n","  if num_labeled > batch_size:\n","    n_labeled_per_class = int(batch_size/n_classes)\n","  else:\n","    n_labeled_per_class = int(num_labeled/n_classes)\n","\n","  #Take everything as unlabeled data\n","  X_unlabeled = X\n","\n","  #Create dataset from tensor slices\n","  features_placeholder_labeled = tf.placeholder(X_labeled.dtype, X_labeled.shape)\n","  features_placeholder = tf.placeholder(X_unlabeled.dtype, X_unlabeled.shape)\n","  labels_placeholder = tf.placeholder(y.dtype, y.shape) #This is for num_labeled == -1\n","  labels_placeholder_labeled = tf.placeholder(y_labeled.dtype, y_labeled.shape)\n","\n","  ds_lab = tf.data.Dataset.from_tensor_slices((features_placeholder_labeled, labels_placeholder_labeled))\n","\n","  ds_unlab = tf.data.Dataset.from_tensor_slices(features_placeholder)\n","  ds_unlab = ds_unlab.shuffle(buffer_size=10, reshuffle_each_iteration=True).batch(batch_size=batch_size-n_labeled_per_class*n_classes, drop_remainder=True).repeat()\n","\n","  ds_full = tf.data.Dataset.from_tensor_slices((features_placeholder,labels_placeholder))\n","  ds_full = ds_full.shuffle(buffer_size=10, reshuffle_each_iteration=True).batch(batch_size=batch_size, drop_remainder=True).repeat()\n","\n","  iterator_full = ds_full.make_initializable_iterator()\n","  iterator_unlab = ds_unlab.make_initializable_iterator()\n","  #Create datasets for each class\n","  datasets = [ds_lab.filter(lambda x,y : tf.equal(y,lab)) for lab in range(n_classes)]\n","  iterators = []\n","  nexts = []\n","\n","  next = ()\n","\n","  if num_labeled != -1:\n","    for idx,d in enumerate(datasets):\n","      datasets[idx] = d.shuffle(buffer_size=10, reshuffle_each_iteration=True).batch(batch_size=n_labeled_per_class, drop_remainder=True).repeat()\n","      iterators =iterators + [datasets[idx].make_initializable_iterator()]\n","      nexts = nexts + [iterators[idx].get_next()]\n","\n","    seed = np.random.randint(100)\n","    X_out = tf.random.shuffle(tf.concat([x[0] for x in nexts],axis=0),seed = seed)\n","    y_out = tf.random.shuffle(tf.concat([x[1] for x in nexts],axis=0),seed = seed)\n","    y_out_again = tf.random.shuffle(tf.concat([x[1] for x in nexts],axis=0),seed = seed)\n","\n","    X_out_un = iterator_unlab.get_next()\n","    y_out_un = tf.constant(shape=([batch_size-n_labeled_per_class*n_classes]), value = -1,dtype=tf.float64)\n","\n","    next = (tf.concat([X_out,X_out_un],axis=0), tf.concat([y_out,y_out_un],axis=0))\n","  else:\n","    next = iterator_full.get_next()\n","\n","  \n","  if num_labeled == -1:\n","    num_labeled = batch_size\n","  \n","  features_placeholder_test = tf.placeholder(X_test.dtype, shape=(None,WND_SZE,WND_SZE,N_CHANNELS),name='X_test')\n","  labels_placeholder_test = tf.placeholder(y_test.dtype, shape=(None,),name='y_test')\n","  \n","  \n","  inputs =  tf.placeholder(tf.float32, shape=(None,WND_SZE,WND_SZE,N_CHANNELS),name='inputs')\n","  outputs = tf.placeholder(tf.float32, shape=(None,),name='outputs')\n","  isTrain = tf.placeholder(tf.bool, shape=())\n","  \n","  \n","  #Gamma and beta initialization: Need one gamma (for softmax) and N+K many with different shapes. \n","  gamma = tf.Variable(tf.ones([N_CLASSES])) #Take the prev. to last one e.g. 90\n","  beta = [tf.Variable(tf.zeros([kernel_size,kernel_size,fs])) for fs in filter_size]+[tf.Variable(tf.zeros([s])) for s in fc]\n","  beta = beta + [tf.Variable(tf.zeros([N_CLASSES]))] #For the last layer\n","  \n","  def usetrain():\n","    inputs = next[0]\n","    outputs = next[1]\n","    return inputs, outputs\n","  def usetest():\n","    return features_placeholder_test, labels_placeholder_test\n","\n","  assert X_test is not None, \"Check if Test data is present in session\"\n","  input, output = tf.cond(isTrain, usetrain, usetest)\n","  \n","  #Helper functions\n","  join = lambda l, u: tf.concat([l, u], axis=0) #Stack in the depth (batch, height, w, depth)\n","  labeled = lambda x: x[:num_labeled_tf] if x is not None else x #Use tf.getitem (implicitly)\n","  unlabeled = lambda x: x[num_labeled_tf:] if x is not None else x\n","  split_lu = lambda x: (labeled(x), unlabeled(x))\n","  \n","  #Running average for the clean pass and the labeled points\n","  ema = tf.train.ExponentialMovingAverage(decay=0.9999)  # to calculate the moving averages of mean and variance\n","  bn_assigns = []\n","  #Initialize with shapes (1,kernel_size, kernel_size, filter_size)\n","  running_mean = [tf.Variable(tf.constant(0.0, shape=[1,kernel_size,kernel_size,f]), trainable=False) for f in filter_size]+[tf.Variable(tf.constant(0.0, shape=[s]), trainable=False) for s in fc]\n","  running_mean = running_mean + [tf.Variable(tf.constant(0.0, shape=[N_CLASSES]))]\n","  running_var = [tf.Variable(tf.constant(1.0, shape=[1,kernel_size,kernel_size,f]), trainable=False) for f in filter_size]+[tf.Variable(tf.constant(1.0, shape=[s]), trainable=False) for s in fc]\n","  running_var = running_var + [tf.Variable(tf.constant(1.0, shape=[N_CLASSES]))]\n","  \n","  \n","  def new_activation_dict():\n","    return AttributeDict({'z': {}, 'h': {}, 's': {}, 'm': {}})\n","  \n","  if shapes[-2][0] == 'conv':\n","    W = tf.Variable(tf.random_normal(shape=[kernel_size**2 * filter_size[-1],N_CLASSES])) #In case the last layer is a conv layer\n","    V = tf.Variable(tf.random_normal(shape=[N_CLASSES, kernel_size**2 * filter_size[-1]])) #Matrix for decoder. Takes the softmax layer shape (?,9) -> (?,kernel_size**2 * filter_size[-1]) to then reshape to a tensor  \n","  else:\n","    W = tf.Variable(tf.random_normal(shape=[shapes[-2][1],N_CLASSES])) #In case the last layer is a fully connected layer.\n","    V = tf.Variable(tf.random_normal(shape=[N_CLASSES,shapes[-2][1]])) #Matrix for decoder. Takes the softmax layer shape (?,9) -> (?,kernel_size**2 * filter_size[-1]) / (?,fully_connected_shape) -> reshape or not\n","  \n","  if K>0:\n","    W_fc = [tf.Variable(tf.random_normal(shape=[kernel_size**2 * filter_size[-1],fc[0]]))] #The first weight matrix for the fc part.\n","    V_fc = [tf.Variable(tf.random_normal(shape=[fc[0],kernel_size**2 * filter_size[-1]]))] #Input dimesnion is fc[0], e.g. fc=[10,20,30], encoder: (?,5,5,30)->(?,10)->(?,20)->(?,30)->(?,9)->(Decoder)(?,30)->(?,20)->(?,10)->(?,5,5,30)\n","    if K>1: #TODO: Works without if? \n","      W_fc = W_fc + [tf.Variable(tf.random_normal(shape=[fc[i-1],fc[i]])) for i in range(1,K)]\n","      V_fc = V_fc + [tf.Variable(tf.random_normal(shape=[fc[i],fc[i-1]])) for i in range(1,K)] #The matrix that \n","      print(V_fc)\n","\n","  \n","  def g(z_lat, u, size):\n","    shape = tf.shape(u)[1:] #Don't take the batch size as a dimension \n","    wi = lambda inits, name: tf.Variable(inits * tf.ones(size), name=name)\n","    a1 = wi(0., 'a1')\n","    a2 = wi(1., 'a2')\n","    a3 = wi(0., 'a3')\n","    a4 = wi(0., 'a4')\n","    a5 = wi(0., 'a5')\n","    a6 = wi(0., 'a6')\n","    a7 = wi(1., 'a7')\n","    a8 = wi(0., 'a8')\n","    a9 = wi(0., 'a9')\n","    a10 = wi(0., 'a10')\n","    mu = a1 * tf.sigmoid(a2 * u + a3) + a4 * u + a5\n","    v = a6 * tf.sigmoid(a7 * u + a8) + a9 * u + a10\n","    z_est = (z_lat - mu) * v + mu\n","    return z_est\n","  \n","  \n","  #Encoder\n","  def encoder(input, noise_std):\n","    with tf.variable_scope('encoder', reuse=tf.AUTO_REUSE):\n","      #Apply noise to the input\n","      h = tf.cast(input,tf.float32) + tf.random_normal(dtype=tf.float32,shape=tf.shape(input)) * noise_std #Normal noise 0 mean 1 std\n","      d = AttributeDict() #This is what we will return. It will contain all the information we need\n","      d.unlabeled = new_activation_dict()\n","      d.labeled = new_activation_dict()\n","      d.unlabeled.z[0] = unlabeled(h)\n","      d.labeled.z[0] = labeled(h)\n","\n","      for i in range(1,L): #Go through the convolutional layers, if we are at i==N+1, we need to flatten and apply W\n","        d.labeled.h[i-1], d.unlabeled.h[i-1] = split_lu(h)\n","        \n","        operation = shapes[i-1][0]\n","        output_shape = shapes[i-1][1]\n","\n","        if operation=='softmax':\n","          z = tf.layers.flatten(h)\n","          z = tf.matmul(z,W)\n","          keep_dims = False\n","        elif operation=='conv':\n","          #Compute new z by applying convolution followed by ReLU after normalization\n","          z = tf.layers.conv2d(h,filters=filter_size[i-1], kernel_size=kernel_size, padding='same')\n","          keep_dims = True\n","        else:\n","          #No need to check for input dim, because flatten preserves batch axis\n","          z = tf.layers.flatten(h)\n","          z = tf.matmul(z,W_fc[i-1-N])\n","          keep_dims = False\n","          \n","        #Shape: (?,5,5,filter_size) or (?,fc_size)\n","        #Normalize\n","        z_lbld, z_unlbld = split_lu(z)\n","\n","        m_unlbld, s_unlbld = tf.nn.moments(z_unlbld, axes=[0], keep_dims=keep_dims) #Compute along depth\n","        m_lbld, s_lbld = tf.nn.moments(z_lbld, axes=[0], keep_dims=keep_dims)\n","        #Shape: (1,5,5,filter_size)\n","\n","        if noise_std == 0: #Clean pass\n","          #Update the running averages and get the mean and variance of the labeled points again\n","          assign_mean = running_mean[i-1].assign(m_lbld)\n","          assign_var = running_var[i-1].assign(s_lbld)\n","          with tf.control_dependencies([assign_mean, assign_var]):\n","            bn_assigns.append(ema.apply([running_mean[i-1], running_var[i-1]]))\n","            m_lbld = ema.average(running_mean[i-1])\n","            s_lbld = ema.average(running_var[i-1])\n","\n","\n","        z = join(\n","          (z_lbld-m_lbld) / tf.sqrt(s_lbld + 1e-10),\n","          (z_unlbld-m_unlbld) / tf.sqrt(s_unlbld + 1e-10))\n","\n","        if noise_std > 0:\n","          z += tf.random_normal(tf.shape(z)) * noise_std\n","\n","        z_lateral = z\n","\n","        if i==L-1: #We need to apply softmax and multiply with gamma\n","          z = tf.add(z,beta[i-1])\n","          z = tf.multiply(z, gamma)\n","          h = tf.nn.softmax(z)\n","        else:  \n","          #Now apply activation. But before we apply the activation, add beta and multiply\n","          #with gamma. Gamma is not used for ReLU. We apply Gamma for the softmax layer.\n","          z += beta[i-1] #i starts at 1, but beta starts at 0\n","          #Apply ReLU\n","          h = tf.nn.relu(z) #h gets assigned at the beginning of the for loop\n","\n","        #Now save the variables: z_lateral, m_unlbld, s_unlbld, h\n","        d.labeled.z[i], d.unlabeled.z[i] = split_lu(z_lateral) #The real z has been compromised\n","        d.unlabeled.s[i] = s_unlbld\n","        d.unlabeled.m[i] = m_unlbld\n","\n","      #Get the last h.\n","      d.labeled.h[i], d.unlabeled.h[i] = split_lu(h)\n","\n","      return h, d\n","   #End encoder\n","  \n","  \n","  #If isTrain is false, use the encoder without the splitting\n","  y_clean, clean = encoder(input, noise_std=0.0)\n","  \n","  #Get the clean run\n","  #y_clean, clean = encoder(input, noise_std=0.0, isTrain=True)\n","  #Get the corrupted encoder run\n","  y_corrupted, corr = encoder(input, noise_std=noise_std)\n","  \n","  #Use this to store the z_est etc.\n","  est = new_activation_dict()\n","  \n","  #Decoder path\n","  filter_dims = [DEPTH] + filter_size\n","  #Start at index N+1 and go through index 0, N=3\n","  cost_recon = []\n","  for i in np.arange(L)[::-1]: #Start from L-1 --> 0, L+1 = N+2 = 6, 30-90-30-15-9\n","    #Get all the information we need\n","    z_corr = corr.unlabeled.z[i]\n","    z_clean = clean.unlabeled.z[i]\n","    if i != 0:\n","      z_clean_s = clean.unlabeled.s[i]\n","      z_clean_m = clean.unlabeled.m[i]\n","      \n","    if i==L-1: #The top level\n","      #Just normalize the (?,9) output\n","      ver = corr.unlabeled.h[i]\n","      size = [N_CLASSES]\n","      keep_dims = False\n","    elif i==L-2: #Apply the matrix V\n","      ver = tf.matmul(est.z.get(i+1), V) #This produces a (?,375)\n","      if K==0: #If we do not have any fully connected layers after this, then reshape\n","        ver = tf.reshape(ver, shape=[-1,WND_SZE,WND_SZE,filter_size[-1]])\n","        size = [WND_SZE, WND_SZE, filter_size[-1]]\n","        keep_dims = True\n","      else:\n","        size = [fc[-1]]\n","        keep_dims = False\n","    else:\n","      #Get the corresponding operation:\n","      operation = shapes[i][0]\n","      print(operation)\n","      if operation == 'conv':\n","        #Deconvolve. This is just a convolution to a new filter size. We leave the kernel size untouched.\n","        ver = tf.layers.conv2d(est.z.get(i+1),filters=filter_dims[i], kernel_size=kernel_size, padding='same')\n","        size = [WND_SZE, WND_SZE, filter_dims[i]]\n","        keep_dims = True\n","      else: #Operation must be to apply the V_fc matrix\n","        tmp = tf.layers.flatten(est.z.get(i+1)) #Flatten. Note: This can bet optimized by checking if we really need to reshape\n","        print(tmp)\n","        print(V_fc[i-N])\n","        ver = tf.matmul(tmp,V_fc[i-N])\n","        if (i-N) == 0: #This was the last fully connected layer, now reshape\n","          ver = tf.reshape(ver, shape=[-1,WND_SZE,WND_SZE,filter_size[-1]])\n","          size = [WND_SZE, WND_SZE, filter_size[-1]]\n","          keep_dims = True\n","        else:\n","          size = [fc[i-1-N]]\n","          keep_dims = False\n","        \n","    print(size)\n","    m, s = tf.nn.moments(ver, axes=[0], keep_dims=keep_dims) #Compute along depth\n","    ver = (ver-m) / tf.sqrt(s + 1e-10)\n","    \n","    #Now apply g to get z_est, g(z_corr_from_encoder, ver (u in the paper))\n","    z_est = g(z_corr, ver, size)\n","    \n","    #Now normalize using the clean mean and clean variance, but only if i != 0\n","    if i != 0:\n","      z_est_norm = (z_est - z_clean_m) / tf.sqrt(z_clean_s + 1e-10)\n","    else:\n","      z_est_norm = z_est\n","    \n","    #Now compute the cost and append the weighted cost. Weigh by the size of the layer and the denoising cost\n","    c_tmp = (tf.reduce_mean(tf.reduce_sum(tf.square(z_est_norm - z_clean), 1)) / tf.cast(tf.reduce_prod(tf.shape(z_est)[1:]),dtype=tf.float32)) * denoising_cost[i]\n","    cost_recon.append(c_tmp)\n","    est.z[i] = z_est_norm  \n","  \n","    \n","  y_corrupted = labeled(y_corrupted)\n","  target = labeled(tf.one_hot(tf.cast(output,tf.int32),depth=N_CLASSES))\n","  target = tf.cast(target, dtype=tf.float32)\n","  yy = labeled(y_clean)  \n","    \n","  with tf.name_scope('supervised_cost'):\n","    supervised_cost = -tf.reduce_mean(tf.reduce_sum(target*tf.log(y_corrupted), 1), name='supervised_cost')\n","  tf.summary.scalar('supervised_cost', supervised_cost)\n","    \n","  with tf.name_scope('unsupervised_cost'):\n","    unsupervised_cost = tf.add_n(cost_recon, name='unsupervised_cost')\n","  tf.summary.scalar('unsupervised_cost', unsupervised_cost)\n","  \n","  with tf.name_scope('total'):  \n","    loss = supervised_cost + unsupervised_cost\n","  tf.summary.scalar('total', loss)\n","\n","   \n","  prediction_cost = -tf.reduce_mean(tf.reduce_sum(target*tf.log(yy), 1),name='pred_cost')\n","  correct_prediction = tf.equal(tf.argmax(yy,1), tf.argmax(target, 1), name='correct_prediction')\n","  with tf.name_scope('accuracy'):\n","    accuracy = tf.multiply(tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32)),tf.constant(100.0),name='accuracy')\n","  tf.summary.scalar('accuracy', accuracy)\n","  \n","  \n","  learning_rate = tf.Variable(lr, trainable=False)\n","  train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","  \n","  # add the updates of batch normalization statistics to train_step\n","  bn_updates = tf.group(*bn_assigns)\n","  with tf.control_dependencies([train_step]):\n","     train_step = tf.group(bn_updates)\n","  \n","  saver = tf.train.Saver()\n","  sess = tf.Session()\n","  \n","  merged = tf.summary.merge_all()\n","  train_writer = tf.summary.FileWriter('./log_ladder/train' , sess.graph)\n","  test_writer = tf.summary.FileWriter('./log_ladder/test')\n","  \n","  sess.run(tf.global_variables_initializer())\n","\n","  \n","  #Initialize the iterators for the data\n","  sess.run(iterator_full.initializer, feed_dict={features_placeholder: X, labels_placeholder:y })\n","\n","  sess.run(iterator_unlab.initializer, feed_dict={features_placeholder: X})\n","  for iterator in iterators:\n","    sess.run(iterator.initializer, feed_dict={features_placeholder_labeled: X_labeled,\n","                                            labels_placeholder_labeled: y_labeled})\n","  \n","  \n","  #Restore checkpoints, if any\n","  i_iter = 0\n","  ckpt = tf.train.get_checkpoint_state('checkpoints/')\n","  if ckpt and ckpt.model_checkpoint_path:\n","    print(\"Found checkpont! Restore...\")\n","    saver.restore(sess, ckpt.model_checkpoint_path)\n","    epoch_n = int(ckpt.model_checkpoint_path.split('-')[1])\n","    i_iter = epoch_n+1\n","    print(\"Restored Epoch %s\" % epoch_n)\n","  else:\n","    print(\"No checkpoint, initialize variables...\")\n","    if not os.path.exists('checkpoints'):\n","      os.makedirs('checkpoints')\n","    sess.run(tf.global_variables_initializer()) # initialization\n","    print(sess.run(tf.report_uninitialized_variables()))\n","  \n","  \n","  \n","  def train_acc():\n","    acc,output = sess.run([accuracy,target], feed_dict={isTrain: False, num_labeled_tf:X_train.shape[0], features_placeholder_test: X_train, labels_placeholder_test:y_train})\n","    return acc\n","  \n","  def test_acc():\n","    acc = sess.run(accuracy, feed_dict={isTrain: False, num_labeled_tf:X_test.shape[0], features_placeholder_test: X_test, labels_placeholder_test:y_test})\n","    return acc\n","  \n","  \n","  n_iter = int(N_EXAMPLES/batch_size)\n","  n_iter = 2\n","  for epoch in range(i_iter,num_epochs):\n","    for i in range(n_iter):\n","      \n","      #Training step. Set num_labeled to the true num_labeled so that we split the data accordingly.\n","      sess.run(train_step, feed_dict={isTrain: True, num_labeled_tf:num_labeled, features_placeholder_test: X_test, labels_placeholder_test:y_test})\n","      \n","      #For accuracy measures, we want to use the clean encoder and the running average mean and std of the labeled points. We thus have to set num_labeled to the\n","      #full test/train size to avoid splitting. Note that we are NOT training in this step, since we are not requesting the 'train_step' operation.\n","      if i % 100 == 0: #Remove this for more fine grained analysis.\n","        summary,_,_ = sess.run([merged,train_step,accuracy], feed_dict={isTrain: True, num_labeled_tf:num_labeled, features_placeholder_test: X_test, labels_placeholder_test:y_test})\n","        train_writer.add_summary(summary, i + epoch*n_iter)\n","        train_writer.flush()\n","      \n","      if i % 100 == 0:\n","        acc = sess.run(accuracy, feed_dict={isTrain: False, num_labeled_tf:X_test.shape[0], features_placeholder_test: X_test, labels_placeholder_test:y_test})\n","        test_writer.add_summary(summary, i + epoch*n_iter)\n","        test_writer.flush()\n","        \n","    print(\"Epoch: %s Train accuracy: %s\" % (epoch, train_acc()))\n","    saver.save(sess, 'checkpoints/model.ckpt', epoch)\n","\n","  print(\"Final test accuracy is: %s\" % test_acc())\n","  sess.close()\n","    \n","  tf.reset_default_graph()\n","  return\n","        \n","\n","def delete_checkpoints():\n","  import shutil\n","  import os\n","  if os.path.exists('checkpoints/') and os.path.isdir('checkpoints/'):\n","      shutil.rmtree('checkpoints/')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-cZhR5wFKW8q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"outputId":"00bf604f-fb87-428f-df08-b583f2990905","executionInfo":{"status":"ok","timestamp":1541365770995,"user_tz":300,"elapsed":7066,"user":{"displayName":"julian buchel","photoUrl":"","userId":"03709653353653158720"}}},"cell_type":"code","source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip"],"execution_count":19,"outputs":[{"output_type":"stream","text":["--2018-11-04 21:09:24--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 34.232.181.106, 34.231.150.116, 34.231.75.48, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|34.232.181.106|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5363700 (5.1M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","ngrok-stable-linux- 100%[===================>]   5.11M  3.51MB/s    in 1.5s    \n","\n","2018-11-04 21:09:27 (3.51 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [5363700/5363700]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"metadata":{"id":"TN7XceMjKbph","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"30eac17d-6174-428e-c0d7-bb034167eafc","executionInfo":{"status":"ok","timestamp":1541367234031,"user_tz":300,"elapsed":4021,"user":{"displayName":"julian buchel","photoUrl":"","userId":"03709653353653158720"}}},"cell_type":"code","source":["LOG_DIR = './log_ladder'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","get_ipython().system_raw('./ngrok http 6006 &')\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":32,"outputs":[{"output_type":"stream","text":["http://09bb5076.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"O5AW9ymQ4_GV","colab_type":"code","colab":{}},"cell_type":"code","source":["delete_checkpoints()\n","\n","train(X_train,y_train,X_test,y_test,num_epochs=10,noise_std=0.1,lr=0.001,filter_size=[90,30,15],fc=[20,20],\n","      denoising_cost=[0.1,0.1,0.1,0.1,0.1,0.1,0.1],num_labeled=300,batch_size=100)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ibzChSEX2-0U","colab_type":"code","colab":{}},"cell_type":"code","source":["import shutil\n","import os\n","if os.path.exists('log_ladder/') and os.path.isdir('log_ladder/'):\n","    shutil.rmtree('log_ladder/')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-v8IcUoQc2v-","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def unpool_2d(pool, \n","              ind, \n","              stride=[1, 2, 2, 1], \n","              scope='unpool_2d'):\n","  \"\"\"Adds a 2D unpooling op.\n","  https://arxiv.org/abs/1505.04366\n","  Unpooling layer after max_pool_with_argmax.\n","       Args:\n","           pool:        max pooled output tensor\n","           ind:         argmax indices\n","           stride:      stride is the same as for the pool\n","       Return:\n","           unpool:    unpooling tensor\n","  \"\"\"\n","  with tf.variable_scope(scope):\n","    input_shape = tf.shape(pool)\n","    output_shape = [input_shape[0], input_shape[1] * stride[1], input_shape[2] * stride[2], input_shape[3]]\n","\n","    flat_input_size = tf.reduce_prod(input_shape)\n","    flat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n","\n","    pool_ = tf.reshape(pool, [flat_input_size])\n","    batch_range = tf.reshape(tf.range(tf.cast(output_shape[0], tf.int64), dtype=ind.dtype), \n","                                      shape=[input_shape[0], 1, 1, 1])\n","    b = tf.ones_like(ind) * batch_range\n","    b1 = tf.reshape(b, [flat_input_size, 1])\n","    ind_ = tf.reshape(ind, [flat_input_size, 1])\n","    ind_ = tf.concat([b1, ind_], 1)\n","\n","    ret = tf.scatter_nd(ind_, pool_, shape=tf.cast(flat_output_shape, tf.int64))\n","    ret = tf.reshape(ret, output_shape)\n","\n","    set_input_shape = pool.get_shape()\n","    set_output_shape = [set_input_shape[0], set_input_shape[1] * stride[1], set_input_shape[2] * stride[2], set_input_shape[3]]\n","    ret.set_shape(set_output_shape)\n","    return ret\n","\n","input = tf.placeholder(dtype = X_test.dtype, shape=X_test.shape)\n","\n","z = tf.layers.conv2d(input,10,[5,5],padding='same') #filter_size, kernel_size\n","shape_z = tf.shape(z)\n","#Do max_pooling\n","output, argmax = tf.nn.max_pool_with_argmax(z,ksize=[1,4,4,1],strides=[1,1,1,1],padding='VALID')\n","shape_pooled = tf.shape(output)\n","\n","unpooled = unpool_2d(output,argmax,stride=[1,1,1,1])\n","shape_unpooled = tf.shape(unpooled)\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","sz,su,sp = sess.run([shape_z, shape_unpooled,shape_pooled],feed_dict={input:X_test})\n","print(sz)\n","print(su)\n","print(sp)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lcSXnNnlD4mP","colab_type":"code","colab":{}},"cell_type":"code","source":["num_labeled = 1000\n","batch_size = 200\n","y = y_train\n","X = X_train\n","  \n","n_classes = len(np.unique(y_test))\n","\n","n_labeled_per_class = int(num_labeled/n_classes)  \n","\n","\n","  \n","#Create X_labeled and X_unlabeled where X_labeled has num_labeled entries which are balanced w.r.t. the class labels\n","indices = np.arange(len(y))\n","i_labeled = []\n","for c in range(n_classes):\n","      i = indices[y==c][:n_labeled_per_class]\n","      i_labeled += list(i)\n","\n","\n","X_labeled = X[i_labeled,:,:,:]\n","y_labeled = y[i_labeled]\n","\n","\n","\n","# If num_labeled == -1, we only create one dataset, else we create one labeled and unlabeled set and balance the batches\n","\n","n_classes = len(np.unique(y_test))\n","\n","if batch_size > num_labeled:\n","  n_labeled_per_class = int(num_labeled/n_classes)\n","else:\n","  n_labeled_per_class = int(batch_size/n_classes)\n","\n","#Take everything as unlabeled data\n","X_unlabeled = X\n","\n","#Create dataset from tensor slices\n","features_placeholder_labeled = tf.placeholder(X_labeled.dtype, X_labeled.shape)\n","features_placeholder = tf.placeholder(X_unlabeled.dtype, X_unlabeled.shape)\n","labels_placeholder = tf.placeholder(y.dtype, y.shape) #This is for num_labeled == -1\n","labels_placeholder_labeled = tf.placeholder(y_labeled.dtype, y_labeled.shape)\n","\n","ds_lab = tf.data.Dataset.from_tensor_slices((features_placeholder_labeled, labels_placeholder_labeled))\n","\n","ds_unlab = tf.data.Dataset.from_tensor_slices(features_placeholder)\n","ds_unlab = ds_unlab.shuffle(buffer_size=10, reshuffle_each_iteration=True).batch(batch_size=batch_size-n_labeled_per_class*n_classes, drop_remainder=True).repeat()\n","\n","ds_full = tf.data.Dataset.from_tensor_slices((features_placeholder,labels_placeholder))\n","ds_full = ds_full.shuffle(buffer_size=10, reshuffle_each_iteration=True).batch(batch_size=batch_size, drop_remainder=True).repeat()\n","\n","iterator_full = ds_full.make_initializable_iterator()\n","iterator_unlab = ds_unlab.make_initializable_iterator()\n","#Create datasets for each class\n","datasets = [ds_lab.filter(lambda x,y : tf.equal(y,lab)) for lab in range(n_classes)]\n","iterators = []\n","nexts = []\n","\n","next = ()\n","\n","if num_labeled != -1:\n","  for idx,d in enumerate(datasets):\n","    datasets[idx] = d.shuffle(buffer_size=10, reshuffle_each_iteration=True).batch(batch_size=n_labeled_per_class, drop_remainder=True).repeat()\n","    iterators =iterators + [datasets[idx].make_initializable_iterator()]\n","    nexts = nexts + [iterators[idx].get_next()]\n","\n","  seed = np.random.randint(100)\n","  X_out = tf.random.shuffle(tf.concat([x[0] for x in nexts],axis=0),seed = seed)\n","  y_out = tf.random.shuffle(tf.concat([x[1] for x in nexts],axis=0),seed = seed)\n","  y_out_again = tf.random.shuffle(tf.concat([x[1] for x in nexts],axis=0),seed = seed)\n","\n","  X_out_un = iterator_unlab.get_next()\n","  #y_out_un = tf.random_uniform(shape=([batch_size-n_labeled_per_class*n_classes]), minval = n_classes+1, maxval = n_classes+2, dtype=tf.float64)\n","  y_out_un = tf.constant(shape=([batch_size-n_labeled_per_class*n_classes]), value = -1,dtype=tf.float64)\n","\n","  next = (tf.concat([X_out,X_out_un],axis=0), tf.concat([y_out,y_out_un],axis=0))\n","else:\n","  next = iterator_full.get_next()\n","\n","  \n","a = next[0]\n","b = next[1]\n","\n","\n","sess = tf.Session()\n","\n","sess.run(tf.global_variables_initializer())\n","\n","#Initialize the iterators for the data\n","sess.run(iterator_full.initializer, feed_dict={features_placeholder: X, labels_placeholder:y })\n","\n","sess.run(iterator_unlab.initializer, feed_dict={features_placeholder: X})\n","for iterator in iterators:\n","  sess.run(iterator.initializer, feed_dict={features_placeholder_labeled: X_labeled,\n","                                          labels_placeholder_labeled: y_labeled})\n","for i in range(10):\n","  c,d = sess.run([a,b])\n","  \n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"EJTuUVEt0ibK","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}