{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ladder_net_cnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"woHpwj0ArpTy","colab_type":"text"},"cell_type":"markdown","source":["# Ladder Network with Convolutional Layers\n","**Input**: M,N,K specifying the general architecture: INPUT -> [[CONV -> RELU]*N -> POOL]*M -> [FC -> RELU]*K -> FC\n","\n","The Ladder network typically has a following layer structure: [Input size, say 700, 1000, 500, 250, 250, 250, 10]"]},{"metadata":{"id":"ix-ac8gfsbiY","colab_type":"text"},"cell_type":"markdown","source":["# Data"]},{"metadata":{"id":"1A_tCHSiroc0","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install attributedict\n","import numpy as np\n","from sklearn.decomposition import PCA\n","import scipy.io as sio\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","import os\n","import random\n","from random import shuffle\n","from skimage.transform import rotate\n","import scipy.ndimage\n","from sklearn.model_selection import train_test_split\n","import scipy\n","\n","def load_pavia():\n","  \n","  !pip install GoogleDriveDownloader\n","  from google_drive_downloader import GoogleDriveDownloader as gdd\n","  gdd.download_file_from_google_drive(file_id='146WN2eZ6Syf-z1KMVRw9GmZdBu_g1JBj',\n","                                    dest_path='./datasets/paviau.mat', unzip=False)\n","\n","  gdd.download_file_from_google_drive(file_id='1L9OoAHnLVmPGbfKx8NhEbugxMzE1PG4j',\n","                                    dest_path='./datasets/paviau_gt.mat', unzip=False)\n","\n","  X = sio.loadmat('./datasets/paviau.mat')['paviaU']\n","  y = sio.loadmat('./datasets/paviau_gt.mat')['paviaU_gt']\n","\n","  return X, y\n","  \n","  \n","def createPatches(X, y, windowSize=5, removeZeroLabels = True):\n","  margin = int((windowSize - 1) / 2)\n","  zeroPaddedX = padWithZeros(X, margin=margin)\n","  # split patches\n","  patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n","  patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n","  patchIndex = 0\n","  for r in range(margin, zeroPaddedX.shape[0] - margin):\n","      for c in range(margin, zeroPaddedX.shape[1] - margin):\n","          patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n","          patchesData[patchIndex, :, :, :] = patch\n","          patchesLabels[patchIndex] = y[r-margin, c-margin]\n","          patchIndex = patchIndex + 1\n","  if removeZeroLabels:\n","      patchesData = patchesData[patchesLabels>0,:,:,:]\n","      patchesLabels = patchesLabels[patchesLabels>0]\n","      patchesLabels -= 1\n","  return patchesData, patchesLabels\n","  \n","  \n","def padWithZeros(X, margin=2):\n","  newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n","  x_offset = margin\n","  y_offset = margin\n","  newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n","  return newX\n","  \n","def standartizeData(X):\n","  newX = np.reshape(X, (-1, X.shape[2]))\n","  print(newX.shape)\n","  scaler = preprocessing.StandardScaler().fit(newX)  \n","  newX = scaler.transform(newX)\n","  newX = np.reshape(newX, (X.shape[0],X.shape[1],X.shape[2]))\n","  return newX, scaler\n","  \n","  \n","def applyPCA(X, numComponents=75):\n","  newX = np.reshape(X, (-1, X.shape[2]))\n","  pca = PCA(n_components=numComponents, whiten=True)\n","  newX = pca.fit_transform(newX)\n","  newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n","  return newX, pca\n","  \n","  \n","def diff(first, second):\n","  second = set(second)\n","  return [item for item in first if item not in second]\n","\n","\n","def splitTrainTestSet(X, y, testRatio=0.10):\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=345,\n","                                                      stratify=y)\n","  return X_train, X_test, y_train, y_test\n","  \n","  \n","def AugmentData(X_train):\n","  for i in range(int(X_train.shape[0]/2)):\n","      patch = X_train[i,:,:,:]\n","      num = random.randint(0,2)\n","      if (num == 0):\n","          flipped_patch = np.flipud(patch)\n","      if (num == 1):\n","          flipped_patch = np.fliplr(patch)\n","      if (num == 2):\n","          no = random.randrange(-180,180,30)\n","          flipped_patch = scipy.ndimage.interpolation.rotate(patch, no,axes=(1, 0),\n","                                                             reshape=False, output=None, order=3, mode='constant', cval=0.0, prefilter=False)\n","\n","      patch2 = flipped_patch\n","      X_train[i,:,:,:] = patch2\n","\n","  return X_train\n","  \n","  \n","def oversampleWeakClasses(X, y):\n","  uniqueLabels, labelCounts = np.unique(y, return_counts=True)\n","  maxCount = np.max(labelCounts)\n","  labelInverseRatios = maxCount / labelCounts  \n","  # repeat for every label and concat\n","  newX = X[y == uniqueLabels[0], :, :, :].repeat(round(labelInverseRatios[0]), axis=0)\n","  newY = y[y == uniqueLabels[0]].repeat(round(labelInverseRatios[0]), axis=0)\n","  for label, labelInverseRatio in zip(uniqueLabels[1:], labelInverseRatios[1:]):\n","      cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n","      cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n","      newX = np.concatenate((newX, cX))\n","      newY = np.concatenate((newY, cY))\n","  np.random.seed(seed=42)\n","  rand_perm = np.random.permutation(newY.shape[0])\n","  newX = newX[rand_perm, :, :, :]\n","  newY = newY[rand_perm]\n","  return newX, newY\n","  \n","  \n","  \n","def savePreprocessedData(X_trainPatches, X_testPatches, y_trainPatches, y_testPatches, windowSize):\n","  \n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","  \n","  with open(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/XtrainWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, X_trainPatches)\n","  with open(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/XtestWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, X_testPatches)\n","  with open(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/ytrainWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, y_trainPatches)\n","  with open(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/ytestWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, y_testPatches)\n","      \n","      \n","      \n","      \n","# Global Variables\n","numComponents = 30\n","windowSize = 5\n","testRatio = 0.25\n","saved = True\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","if saved == False:\n","  X, y = load_pavia()\n","  X,_ = standartizeData(X)\n","  X,pca = applyPCA(X,numComponents=numComponents)\n","  XPatches, yPatches = createPatches(X, y, windowSize=windowSize)\n","  X_train, X_test, y_train, y_test = splitTrainTestSet(XPatches, yPatches, testRatio)\n","  print(X_train.shape)\n","  X_train, y_train = oversampleWeakClasses(X_train, y_train)\n","  X_train = AugmentData(X_train)\n","  savePreprocessedData(X_train, X_test, y_train, y_test, windowSize = windowSize)\n","  print(X_train.shape)\n","  \n","else:\n","  X_train = np.load(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/XtrainWindowSize\" + str(windowSize) + \".npy\")\n","  y_train = np.load(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/ytrainWindowSize\" + str(windowSize) + \".npy\")\n","  X_test = np.load(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/XtestWindowSize\" + str(windowSize) + \".npy\")\n","  y_test = np.load(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/ytestWindowSize\" + str(windowSize) + \".npy\")\n","  print(X_train.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IBRmrQQUy6D6","colab_type":"text"},"cell_type":"markdown","source":["# Conv. Ladder Net\n","Architecture: INPUT -> [[CONV -> RELU]*N -> POOL]*M -> [FC -> RELU]*K -> FC\n","\n","**Params**: N,M,K,filter_size (Array of length N)\n","\n","**Default**: N=3,M=0,K=1, filter_size=[3*PCA_comp=90, 30, 15]\n","\n","**For now use only convolution**\n"]},{"metadata":{"id":"FGFqQIyxsOYM","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","from attributedict.collections import AttributeDict\n","\n","def train(X,y,X_test=None,y_test=None,N=3,filter_size=[90,30,15],kernel_size=5,\n","          denoising_cost=[10,1,0.1,0.1,0.1],num_epochs=150,batch_size=200,num_labeled=100,noise_std=0.3,lr=0.02,\n","          decay_after=15):\n","  \n","  tf.reset_default_graph()\n","  tf.set_random_seed(12345)\n","  #We double the batch size here. This has the advantage that in case num_labeled is -1 (use all labels) we can use half of the\n","  #batch size for the clean encoder and the other half for the unsupervised run\n","  \n","  if num_labeled == -1:\n","    num_labeled = batch_size\n","  batch_size *= 2\n"," \n","  \n","  #For now\n","  N = len(filter_size)\n","  \n","  #Shape of X: (?,WND_SZE,WND_SZE, N_CHANNELS)\n","  WND_SZE = X.shape[1]\n","  N_CHANNELS = X.shape[3]\n","  N_CLASSES = len(np.unique(y))\n","  N_EXAMPLES = X.shape[0]\n","  DEPTH = X.shape[-1]\n","  \n","  num_labeled_tf = tf.placeholder(tf.int32, shape=())\n","  \n","  features_placeholder = tf.placeholder(X.dtype, X.shape)\n","  labels_placeholder = tf.placeholder(y.dtype, y.shape)\n","  \n","  features_placeholder_test = tf.placeholder(X_test.dtype, shape=(None,WND_SZE,WND_SZE,N_CHANNELS),name='X_test')\n","  labels_placeholder_test = tf.placeholder(y_test.dtype, shape=(None,),name='y_test')\n","  \n","  \n","  inputs =  tf.placeholder(tf.float32, shape=(None,WND_SZE,WND_SZE,N_CHANNELS),name='inputs')\n","  outputs = tf.placeholder(tf.float32, shape=(None,),name='outputs')\n","  isTrain = tf.placeholder(tf.bool, shape=())\n","  \n","  \n","  #Gamma and beta initialization: Need one gamma (for softmax) and N+K many with different shapes. \n","  gamma = tf.Variable(tf.ones([N_CLASSES])) #Take the prev. to last one e.g. 90\n","  beta = [tf.Variable(tf.zeros([kernel_size,kernel_size,filter_size[l]])) for l in range(N)] \n","  beta = beta + [tf.Variable(tf.zeros([N_CLASSES]))] #For the last layer\n","  \n","  #Prepare a tensorflow dataset\n","  ds = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n","  ds = ds.shuffle(buffer_size=10, reshuffle_each_iteration=True).batch(batch_size=batch_size, drop_remainder=True).repeat()\n","   \n","  iterator = ds.make_initializable_iterator()\n","  next = iterator.get_next()\n","  \n","  def usetrain():\n","    inputs = next[0]\n","    outputs = next[1]\n","    return inputs, outputs\n","  def usetest():\n","    return features_placeholder_test, labels_placeholder_test\n","\n","  assert X_test is not None, \"Check if Test data is present in session\"\n","  input, output = tf.cond(isTrain, usetrain, usetest)\n","  \n","  #Helper functions\n","  join = lambda l, u: tf.concat([l, u], axis=0) #Stack in the depth (batch, height, w, depth)\n","  labeled = lambda x: x[:num_labeled_tf] if x is not None else x #Use tf.getitem (implicitly)\n","  unlabeled = lambda x: x[num_labeled_tf:] if x is not None else x\n","  split_lu = lambda x: (labeled(x), unlabeled(x))\n","  \n","  #Running average for the clean pass and the labeled points\n","  ema = tf.train.ExponentialMovingAverage(decay=0.9999)  # to calculate the moving averages of mean and variance\n","  bn_assigns = []\n","  #Initialize with shapes (1,kernel_size, kernel_size, filter_size)\n","  running_mean = [tf.Variable(tf.constant(0.0, shape=[1,kernel_size,kernel_size,f]), trainable=False) for f in filter_size]\n","  running_mean = running_mean + [tf.Variable(tf.constant(0.0, shape=[N_CLASSES]))]\n","  running_var = [tf.Variable(tf.constant(1.0, shape=[1,kernel_size,kernel_size,f]), trainable=False) for f in filter_size]\n","  running_var = running_var + [tf.Variable(tf.constant(1.0, shape=[N_CLASSES]))]\n","  \n","  \n","  def new_activation_dict():\n","    return AttributeDict({'z': {}, 'h': {}, 's': {}, 'm': {}})\n"," \n","  W = tf.Variable(tf.random_normal(shape=[kernel_size**2 * filter_size[-1],N_CLASSES]))\n","  V = tf.Variable(tf.random_normal(shape=[N_CLASSES, kernel_size**2 * filter_size[-1]]))\n","  \n","  L = N+2 #Convs+In+Soft\n","  \n","  \n","  def g(z_lat, u, size):\n","    shape = tf.shape(u)[1:] #Don't take the batch size as a dimension \n","    wi = lambda inits, name: tf.Variable(inits * tf.ones(size), name=name)\n","    a1 = wi(0., 'a1')\n","    a2 = wi(1., 'a2')\n","    a3 = wi(0., 'a3')\n","    a4 = wi(0., 'a4')\n","    a5 = wi(0., 'a5')\n","    a6 = wi(0., 'a6')\n","    a7 = wi(1., 'a7')\n","    a8 = wi(0., 'a8')\n","    a9 = wi(0., 'a9')\n","    a10 = wi(0., 'a10')\n","    mu = a1 * tf.sigmoid(a2 * u + a3) + a4 * u + a5\n","    v = a6 * tf.sigmoid(a7 * u + a8) + a9 * u + a10\n","    z_est = (z_lat - mu) * v + mu\n","    return z_est\n","  \n","  #Encoder\n","  def encoder(input, noise_std):\n","    with tf.variable_scope('encoder', reuse=tf.AUTO_REUSE):\n","      #Apply noise to the input\n","      h = tf.cast(input,tf.float32) + tf.random_normal(dtype=tf.float32,shape=tf.shape(input)) * noise_std #Normal noise 0 mean 1 std\n","      d = AttributeDict() #This is what we will return. It will contain all the information we need\n","      d.unlabeled = new_activation_dict()\n","      d.labeled = new_activation_dict()\n","      d.unlabeled.z[0] = unlabeled(h)\n","      d.labeled.z[0] = labeled(h)\n","\n","      for i in range(1,L): #Go through the convolutional layers, if we are at i==N+1, we need to flatten and apply W\n","        d.labeled.h[i-1], d.unlabeled.h[i-1] = split_lu(h)\n","\n","        if i==L-1:\n","          z = tf.layers.flatten(h)\n","          z = tf.matmul(z,W)\n","        else:\n","          #Compute new z by applying convolution followed by ReLU after normalization\n","          z = tf.layers.conv2d(h,filters=filter_size[i-1], kernel_size=kernel_size, padding='same')\n","        #Shape: (?,5,5,filter_size)\n","        #Normalize\n","        z_lbld, z_unlbld = split_lu(z)\n","\n","        keep_dims = True\n","        if i==L-1:\n","          keep_dims = False\n","\n","        m_unlbld, s_unlbld = tf.nn.moments(z_unlbld, axes=[0], keep_dims=keep_dims) #Compute along depth\n","        m_lbld, s_lbld = tf.nn.moments(z_lbld, axes=[0], keep_dims=keep_dims)\n","        #Shape: (1,5,5,filter_size)\n","\n","        if noise_std == 0: #Clean pass\n","          #Update the running averages and get the mean and variance of the labeled points again\n","          assign_mean = running_mean[i-1].assign(m_lbld)\n","          assign_var = running_var[i-1].assign(s_lbld)\n","          with tf.control_dependencies([assign_mean, assign_var]):\n","            bn_assigns.append(ema.apply([running_mean[i-1], running_var[i-1]]))\n","            m_lbld = ema.average(running_mean[i-1])\n","            s_lbld = ema.average(running_var[i-1])\n","\n","\n","        z = join(\n","          (z_lbld-m_lbld) / tf.sqrt(s_lbld + 1e-10),\n","          (z_unlbld-m_unlbld) / tf.sqrt(s_unlbld + 1e-10))\n","\n","        if noise_std > 0:\n","          z += tf.random_normal(tf.shape(z)) * noise_std\n","\n","        z_lateral = z\n","\n","        if i==L-1: #We need to apply softmax and multiply with gamma\n","          z = tf.add(z,beta[i-1])\n","          z = tf.multiply(z, gamma)\n","          h = tf.nn.softmax(z)\n","        else:  \n","          #Now apply activation. But before we apply the activation, add beta and multiply\n","          #with gamma. Gamma is not used for ReLU. We apply Gamma for the softmax layer.\n","          z += beta[i-1] #i starts at 1, but beta starts at 0\n","          #Apply ReLU\n","          h = tf.nn.relu(z) #h gets assigned at the beginning of the for loop\n","\n","        #Now save the variables: z_lateral, m_unlbld, s_unlbld, h\n","        d.labeled.z[i], d.unlabeled.z[i] = split_lu(z_lateral) #The real z has been compromised\n","        d.unlabeled.s[i] = s_unlbld\n","        d.unlabeled.m[i] = m_unlbld\n","\n","      #Get the last h.\n","      d.labeled.h[i], d.unlabeled.h[i] = split_lu(h)\n","\n","      return h, d\n","   #End encoder\n","  \n","  \n","  #If isTrain is false, use the encoder without the splitting\n","  y_clean, clean = encoder(input, noise_std=0.0)\n","  \n","  #Get the clean run\n","  #y_clean, clean = encoder(input, noise_std=0.0, isTrain=True)\n","  #Get the corrupted encoder run\n","  y_corrupted, corr = encoder(input, noise_std=noise_std)\n","  \n","  #Use this to store the z_est etc.\n","  est = new_activation_dict()\n","  \n","  #Decoder path\n","  filter_dims = [DEPTH] + filter_size\n","  #Start at index N+1 and go through index 0, N=3\n","  cost_recon = []\n","  for i in np.arange(L)[::-1]: #Start from L-1 --> 0, L+1 = N+2 = 6, 30-90-30-15-9\n","    #Get all the information we need\n","    z_corr = corr.unlabeled.z[i]\n","    z_clean = clean.unlabeled.z[i]\n","    #We don't have running mean and variance for the already standardized input. TODO: Correct?\n","    if i != 0:\n","      z_clean_s = clean.unlabeled.s[i]\n","      z_clean_m = clean.unlabeled.m[i]\n","      \n","    if i==L-1: #The top level\n","      #Just normalize the (100,9) output\n","      ver = corr.unlabeled.h[i]\n","    elif i==L-2: #Apply the matrix V\n","      ver = tf.matmul(est.z.get(i+1), V) #This produces a (100,375)\n","      #Now reshape to [batch_size, kernel, kernel, filter[-1]]\n","      ver = tf.reshape(ver, shape=[-1,WND_SZE,WND_SZE,filter_size[-1]])\n","      #ver = tf.reshape(ver, shape=corr.unlabeled.z[N].shape)\n","    else:\n","      #Deconvolve. This is just a convolution to a new filter size. We leave the kernel size untouched.\n","      ver = tf.layers.conv2d(est.z.get(i+1),filters=filter_dims[i], kernel_size=kernel_size, padding='same')\n","    \n","    #Now normalize them\n","    keep_dims = True\n","    if i==L-1: #Only for the first dimension don't keep the dimension since it is (?,), not (?,5,5,?)\n","      keep_dims = False\n","      size = [N_CLASSES]\n","    else: size = [WND_SZE, WND_SZE, filter_dims[i]]\n","        \n","    m, s = tf.nn.moments(ver, axes=[0], keep_dims=keep_dims) #Compute along depth\n","    ver = (ver-m) / tf.sqrt(s + 1e-10)\n","    \n","    #Now apply g to get z_est, g(z_corr_from_encoder, ver (u in the paper))\n","    z_est = g(z_corr, ver, size)\n","    \n","    #Now normalize using the clean mean and clean variance, but only if i != 0\n","    if i != 0:\n","      z_est_norm = (z_est - z_clean_m) / tf.sqrt(z_clean_s + 1e-10)\n","    else:\n","      z_est_norm = z_est\n","    \n","    #Now compute the cost and append the weighted cost. Weigh by the size of the layer and the denoising cost\n","    c_tmp = (tf.reduce_mean(tf.reduce_sum(tf.square(z_est_norm - z_clean), 1)) / tf.cast(tf.reduce_prod(tf.shape(z_est)[1:]),dtype=tf.float32)) * denoising_cost[i]\n","    \n","    cost_recon.append(c_tmp)\n","    est.z[i] = z_est_norm  \n","  \n","    \n","  y_corrupted = labeled(y_corrupted)\n","  target = labeled(tf.one_hot(tf.cast(output,tf.int32),depth=N_CLASSES))\n","  target = tf.cast(target, dtype=tf.float32)\n","  yy = labeled(y_clean)  \n","    \n","  with tf.name_scope('supervised_cost'):\n","    supervised_cost = -tf.reduce_mean(tf.reduce_sum(target*tf.log(y_corrupted), 1), name='supervised_cost')\n","  tf.summary.scalar('supervised_cost', supervised_cost)\n","    \n","  with tf.name_scope('unsupervised_cost'):\n","    unsupervised_cost = tf.add_n(cost_recon, name='unsupervised_cost')\n","  tf.summary.scalar('unsupervised_cost', unsupervised_cost)\n","  \n","  with tf.name_scope('total'):  \n","    loss = supervised_cost + unsupervised_cost\n","  tf.summary.scalar('total', loss)\n","\n","   \n","  prediction_cost = -tf.reduce_mean(tf.reduce_sum(target*tf.log(yy), 1),name='pred_cost')\n","  correct_prediction = tf.equal(tf.argmax(yy,1), tf.argmax(target, 1), name='correct_prediction')\n","  with tf.name_scope('accuracy'):\n","    accuracy = tf.multiply(tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32)),tf.constant(100.0),name='accuracy')\n","  tf.summary.scalar('accuracy', accuracy)\n","  \n","  \n","  learning_rate = tf.Variable(lr, trainable=False)\n","  train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","  \n","  # add the updates of batch normalization statistics to train_step\n","  bn_updates = tf.group(*bn_assigns)\n","  with tf.control_dependencies([train_step]):\n","     train_step = tf.group(bn_updates)\n","  \n","  saver = tf.train.Saver()\n","  sess = tf.Session()\n","  \n","  merged = tf.summary.merge_all()\n","  train_writer = tf.summary.FileWriter('./log_ladder/train' , sess.graph)\n","  test_writer = tf.summary.FileWriter('./log_ladder/test')\n","  \n","  sess.run(tf.global_variables_initializer())\n","\n","  \n","  #Initialize the iterator for the data\n","  sess.run(iterator.initializer, feed_dict={features_placeholder: X,\n","                                          labels_placeholder: y})\n","  \n","  \n","  #Restore checkpoints, if any\n","  i_iter = 0\n","  ckpt = tf.train.get_checkpoint_state('checkpoints/')\n","  if ckpt and ckpt.model_checkpoint_path:\n","    print(\"Found checkpont! Restore...\")\n","    saver.restore(sess, ckpt.model_checkpoint_path)\n","    epoch_n = int(ckpt.model_checkpoint_path.split('-')[1])\n","    i_iter = epoch_n+1\n","    print(\"Restored Epoch %s\" % epoch_n)\n","  else:\n","    print(\"No checkpoint, initialize variables...\")\n","    if not os.path.exists('checkpoints'):\n","      os.makedirs('checkpoints')\n","    sess.run(tf.global_variables_initializer()) # initialization\n","    print(sess.run(tf.report_uninitialized_variables()))\n","  \n","  \n","  \n","  def train_acc():\n","    acc,output = sess.run([accuracy,target], feed_dict={isTrain: False, num_labeled_tf:X_train.shape[0], features_placeholder_test: X_train, labels_placeholder_test:y_train})\n","    return acc\n","  \n","  \n","  n_iter = int(N_EXAMPLES/batch_size)\n","  for epoch in range(i_iter,num_epochs):\n","    for i in range(n_iter):\n","      \n","      #Training step. Set num_labeled to the true num_labeled so that we split the data accordingly.\n","      sess.run(train_step, feed_dict={isTrain: True, num_labeled_tf:num_labeled, features_placeholder_test: X_test, labels_placeholder_test:y_test})\n","      \n","      #For accuracy measures, we want to use the clean encoder and the running average mean and std of the labeled points. We thus have to set num_labeled to the\n","      #full test/train size to avoid splitting. Note that we are NOT training in this step, since we are not requesting the 'train_step' operation.\n","      if i % 100 == 0: #Remove this for more fine grained analysis.\n","        summary,_,_ = sess.run([merged,train_step,accuracy], feed_dict={isTrain: True, num_labeled_tf:num_labeled, features_placeholder_test: X_test, labels_placeholder_test:y_test})\n","        train_writer.add_summary(summary, i + epoch*n_iter)\n","        train_writer.flush()\n","      \n","      if i % 100 == 0:\n","        summary, t = sess.run([merged,target], feed_dict={isTrain: False, num_labeled_tf:X_test.shape[0], features_placeholder_test: X_test, labels_placeholder_test:y_test})\n","        test_writer.add_summary(summary, i + epoch*n_iter)\n","        test_writer.flush()\n","    print(\"Epoch: %s Train accuracy: %s\" % (epoch, train_acc()))\n","    saver.save(sess, 'checkpoints/model.ckpt', epoch)\n","\n","  acc = sess.run(accuracy, feed_dict={isTrain: False, num_labeled_tf:X_test.shape[0], features_placeholder_test: X_test, labels_placeholder_test:y_test})\n","  print(\"Final test accuracy is: %s\" % acc)\n","  sess.close()\n","    \n","  tf.reset_default_graph()\n","  return\n","        \n","\n","def delete_checkpoints():\n","  import shutil\n","  import os\n","  if os.path.exists('checkpoints/') and os.path.isdir('checkpoints/'):\n","      shutil.rmtree('checkpoints/')\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"-cZhR5wFKW8q","colab_type":"code","colab":{}},"cell_type":"code","source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TN7XceMjKbph","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6cceec50-3fec-4a1c-af95-8dbfda2fcbdc","executionInfo":{"status":"ok","timestamp":1541094186313,"user_tz":240,"elapsed":5030,"user":{"displayName":"julian buchel","photoUrl":"","userId":"03709653353653158720"}}},"cell_type":"code","source":["LOG_DIR = './log_ladder'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","get_ipython().system_raw('./ngrok http 6006 &')\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":89,"outputs":[{"output_type":"stream","text":["http://e00a68b0.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"O5AW9ymQ4_GV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"outputId":"8830f7a9-d66f-473a-fc03-e0e32e3f7b38","executionInfo":{"status":"ok","timestamp":1541094530350,"user_tz":240,"elapsed":344013,"user":{"displayName":"julian buchel","photoUrl":"","userId":"03709653353653158720"}}},"cell_type":"code","source":["delete_checkpoints()\n","stds = [0.01,0.1,0.3]\n","\n","train(X_train,y_train,X_test,y_test,num_epochs=10,noise_std=0.1,lr=0.001,filter_size=[90,30,15],\n","      denoising_cost=[0.0,0.0,0.0,0.0,0.0],num_labeled=-1,batch_size=100)"],"execution_count":90,"outputs":[{"output_type":"stream","text":["No checkpoint, initialize variables...\n","[]\n","(128051, 9)\n","Epoch: 0 Train accuracy: 73.106804\n","(128051, 9)\n","Epoch: 1 Train accuracy: 74.241516\n","(128051, 9)\n","Epoch: 2 Train accuracy: 75.1271\n","(128051, 9)\n","Epoch: 3 Train accuracy: 70.48598\n","(128051, 9)\n","Epoch: 4 Train accuracy: 80.522606\n","(128051, 9)\n","Epoch: 5 Train accuracy: 97.081635\n","(128051, 9)\n","Epoch: 6 Train accuracy: 90.7943\n","(128051, 9)\n","Epoch: 7 Train accuracy: 88.21329\n","(128051, 9)\n","Epoch: 8 Train accuracy: 98.47092\n","(128051, 9)\n","Epoch: 9 Train accuracy: 99.156586\n","Final test accuracy is: 99.22386\n"],"name":"stdout"}]},{"metadata":{"id":"ibzChSEX2-0U","colab_type":"code","colab":{}},"cell_type":"code","source":["import shutil\n","import os\n","if os.path.exists('log_ladder/') and os.path.isdir('log_ladder/'):\n","    shutil.rmtree('log_ladder/')"],"execution_count":0,"outputs":[]}]}