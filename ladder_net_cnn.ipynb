{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ladder_net_cnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"woHpwj0ArpTy","colab_type":"text"},"cell_type":"markdown","source":["# Ladder Network with Convolutional Layers\n","**Input**: M,N,K specifying the general architecture: INPUT -> [[CONV -> RELU]*N -> POOL]*M -> [FC -> RELU]*K -> FC\n","\n","The Ladder network typically has a following layer structure: [Input size, say 700, 1000, 500, 250, 250, 250, 10]"]},{"metadata":{"id":"ix-ac8gfsbiY","colab_type":"text"},"cell_type":"markdown","source":["# Data"]},{"metadata":{"id":"1A_tCHSiroc0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":326},"outputId":"adc90cbe-a5cf-4177-e20f-3c9378592574","executionInfo":{"status":"ok","timestamp":1539383928621,"user_tz":240,"elapsed":12095,"user":{"displayName":"julian buchel","photoUrl":"","userId":"03709653353653158720"}}},"cell_type":"code","source":["!pip install attributedict\n","import numpy as np\n","from sklearn.decomposition import PCA\n","import scipy.io as sio\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","import os\n","import random\n","from random import shuffle\n","from skimage.transform import rotate\n","import scipy.ndimage\n","from sklearn.model_selection import train_test_split\n","import scipy\n","\n","def load_pavia():\n","  \n","  !pip install GoogleDriveDownloader\n","  from google_drive_downloader import GoogleDriveDownloader as gdd\n","  gdd.download_file_from_google_drive(file_id='146WN2eZ6Syf-z1KMVRw9GmZdBu_g1JBj',\n","                                    dest_path='./datasets/paviau.mat', unzip=False)\n","\n","  gdd.download_file_from_google_drive(file_id='1L9OoAHnLVmPGbfKx8NhEbugxMzE1PG4j',\n","                                    dest_path='./datasets/paviau_gt.mat', unzip=False)\n","\n","  X = sio.loadmat('./datasets/paviau.mat')['paviaU']\n","  y = sio.loadmat('./datasets/paviau_gt.mat')['paviaU_gt']\n","\n","  return X, y\n","  \n","  \n","def createPatches(X, y, windowSize=5, removeZeroLabels = True):\n","  margin = int((windowSize - 1) / 2)\n","  zeroPaddedX = padWithZeros(X, margin=margin)\n","  # split patches\n","  patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n","  patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n","  patchIndex = 0\n","  for r in range(margin, zeroPaddedX.shape[0] - margin):\n","      for c in range(margin, zeroPaddedX.shape[1] - margin):\n","          patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n","          patchesData[patchIndex, :, :, :] = patch\n","          patchesLabels[patchIndex] = y[r-margin, c-margin]\n","          patchIndex = patchIndex + 1\n","  if removeZeroLabels:\n","      patchesData = patchesData[patchesLabels>0,:,:,:]\n","      patchesLabels = patchesLabels[patchesLabels>0]\n","      patchesLabels -= 1\n","  return patchesData, patchesLabels\n","  \n","  \n","def padWithZeros(X, margin=2):\n","  newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n","  x_offset = margin\n","  y_offset = margin\n","  newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n","  return newX\n","  \n","def standartizeData(X):\n","  newX = np.reshape(X, (-1, X.shape[2]))\n","  scaler = preprocessing.StandardScaler().fit(newX)  \n","  newX = scaler.transform(newX)\n","  newX = np.reshape(newX, (X.shape[0],X.shape[1],X.shape[2]))\n","  return newX, scaler\n","  \n","  \n","def applyPCA(X, numComponents=75):\n","  newX = np.reshape(X, (-1, X.shape[2]))\n","  pca = PCA(n_components=numComponents, whiten=True)\n","  newX = pca.fit_transform(newX)\n","  newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n","  return newX, pca\n","  \n","  \n","def diff(first, second):\n","  second = set(second)\n","  return [item for item in first if item not in second]\n","\n","\n","def splitTrainTestSet(X, y, testRatio=0.10):\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=345,\n","                                                      stratify=y)\n","  return X_train, X_test, y_train, y_test\n","  \n","  \n","def AugmentData(X_train):\n","  for i in range(int(X_train.shape[0]/2)):\n","      patch = X_train[i,:,:,:]\n","      num = random.randint(0,2)\n","      if (num == 0):\n","          flipped_patch = np.flipud(patch)\n","      if (num == 1):\n","          flipped_patch = np.fliplr(patch)\n","      if (num == 2):\n","          no = random.randrange(-180,180,30)\n","          flipped_patch = scipy.ndimage.interpolation.rotate(patch, no,axes=(1, 0),\n","                                                             reshape=False, output=None, order=3, mode='constant', cval=0.0, prefilter=False)\n","\n","      patch2 = flipped_patch\n","      X_train[i,:,:,:] = patch2\n","\n","  return X_train\n","  \n","  \n","def oversampleWeakClasses(X, y):\n","  uniqueLabels, labelCounts = np.unique(y, return_counts=True)\n","  maxCount = np.max(labelCounts)\n","  labelInverseRatios = maxCount / labelCounts  \n","  # repeat for every label and concat\n","  newX = X[y == uniqueLabels[0], :, :, :].repeat(round(labelInverseRatios[0]), axis=0)\n","  newY = y[y == uniqueLabels[0]].repeat(round(labelInverseRatios[0]), axis=0)\n","  for label, labelInverseRatio in zip(uniqueLabels[1:], labelInverseRatios[1:]):\n","      cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n","      cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n","      newX = np.concatenate((newX, cX))\n","      newY = np.concatenate((newY, cY))\n","  np.random.seed(seed=42)\n","  rand_perm = np.random.permutation(newY.shape[0])\n","  newX = newX[rand_perm, :, :, :]\n","  newY = newY[rand_perm]\n","  return newX, newY\n","  \n","  \n","  \n","def savePreprocessedData(X_trainPatches, X_testPatches, y_trainPatches, y_testPatches, windowSize, numPCAComponents = 0, testRatio = 0.25):\n","  \n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","  \n","  with open(\"/content/gdrive/My Drive/colab/preprocessedData/XtrainWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, X_trainPatches)\n","  with open(\"/content/gdrive/My Drive/colab/preprocessedData/XtestWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, X_testPatches)\n","  with open(\"/content/gdrive/My Drive/colab/preprocessedData/ytrainWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, y_trainPatches)\n","  with open(\"/content/gdrive/My Drive/colab/preprocessedData/ytestWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, y_testPatches)\n","      \n","      \n","      \n","      \n","# Global Variables\n","numComponents = 30\n","windowSize = 5\n","testRatio = 0.25\n","saved = True\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","if saved != True:\n","  X, y = load_pavia()\n","  X,pca = applyPCA(X,numComponents=numComponents)\n","  XPatches, yPatches = createPatches(X, y, windowSize=windowSize)\n","  X_train, X_test, y_train, y_test = splitTrainTestSet(XPatches, yPatches, testRatio)\n","  X_train, y_train = oversampleWeakClasses(X_train, y_train)\n","  X_train = AugmentData(X_train)\n","  savePreprocessedData(X_train, X_test, y_train, y_test, windowSize = windowSize, \n","                      numPCAComponents = numComponents,testRatio = testRatio)\n","  print(X_train.shape)\n","  \n","else:\n","  X_train = np.load(\"/content/gdrive/My Drive/colab/preprocessedData/XtrainWindowSize\" + str(windowSize) + \".npy\")\n","  y_train = np.load(\"/content/gdrive/My Drive/colab/preprocessedData/ytrainWindowSize\" + str(windowSize) + \".npy\")\n","  X_test = np.load(\"/content/gdrive/My Drive/colab/preprocessedData/XtestWindowSize\" + str(windowSize) + \".npy\")\n","  y_test = np.load(\"/content/gdrive/My Drive/colab/preprocessedData/ytestWindowSize\" + str(windowSize) + \".npy\")\n","  print(X_train.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: attributedict in /usr/local/lib/python3.6/dist-packages (0.1.8)\n","Requirement already satisfied: tox>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from attributedict) (3.5.2)\n","Requirement already satisfied: easypackage>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from attributedict) (0.1.8)\n","Requirement already satisfied: deepdiff>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from attributedict) (3.3.0)\n","Requirement already satisfied: colour-runner>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from attributedict) (0.1.1)\n","Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from attributedict) (2.2.0)\n","Requirement already satisfied: virtualenv>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (16.0.0)\n","Requirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (0.10.0)\n","Requirement already satisfied: setuptools>=30.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (39.1.0)\n","Requirement already satisfied: pluggy<1,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (0.7.1)\n","Requirement already satisfied: py<2,>=1.4.17 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (1.7.0)\n","Requirement already satisfied: filelock<4,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (3.0.9)\n","Requirement already satisfied: six<2,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (1.11.0)\n","Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from deepdiff>=3.3.0->attributedict) (1.0)\n","Requirement already satisfied: blessings in /usr/local/lib/python3.6/dist-packages (from colour-runner>=0.0.5->attributedict) (1.7)\n","Mounted at /content/gdrive\n","(128051, 5, 5, 30)\n"],"name":"stdout"}]},{"metadata":{"id":"IBRmrQQUy6D6","colab_type":"text"},"cell_type":"markdown","source":["# Conv. Ladder Net\n","Architecture: INPUT -> [[CONV -> RELU]*N -> POOL]*M -> [FC -> RELU]*K -> FC\n","\n","**Params**: N,M,K,filter_size (Array of length N)\n","\n","**Default**: N=3,M=0,K=1, filter_size=[3*PCA_comp=90, 30, 15]\n","\n","**For now use only convolution**\n"]},{"metadata":{"id":"FGFqQIyxsOYM","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","from attributedict.collections import AttributeDict\n","\n","def train(X,y,X_test=None,y_test=None,N=3,filter_size=[90,30,15],kernel_size=5,\n","          denoising_cost=[10,1,0.1,0.1,0.1],num_epochs=150,batch_size=200,num_labeled=100,noise_std=0.3,lr=0.02,\n","          decay_after=15):\n","  \n","  tf.set_random_seed(12345)\n","  \n","  #Shape of X: (?,WND_SZE,WND_SZE, N_CHANNELS)\n","  WND_SZE = X.shape[1]\n","  N_CHANNELS = X.shape[3]\n","  N_CLASSES = len(np.unique(y))\n","  #N_CLASSES = 9 #Hardcoded for Pavia Un. and Salinas dataset\n","  N_EXAMPLES = X.shape[0]\n","  DEPTH = X.shape[-1]\n","  \n","  #Reset the tf default graph\n","  tf.reset_default_graph()\n","  \n","  inputs =  tf.placeholder(tf.float32, shape=(None,WND_SZE,WND_SZE,N_CHANNELS),name='inputs')\n","  outputs = tf.placeholder(tf.float32, shape=(None,),name='outputs')\n","  isTrain = tf.placeholder(tf.bool, shape=())\n","  \n","  \n","  #Gamma and beta initialization: Need one gamma (for softmax) and N+K many with different shapes. \n","  gamma = tf.Variable(tf.ones([N_CLASSES])) #Take the prev. to last one e.g. 90\n","  beta = [tf.Variable(tf.zeros([batch_size,kernel_size,kernel_size,filter_size[l]])) for l in range(N)]  \n","  beta = beta + [tf.Variable(tf.zeros([N_CLASSES]))] #For the last layer\n","  \n","  #Prepare a tensorflow dataset\n","  ds = tf.data.Dataset.from_tensor_slices((X, y))\n","  ds = ds.shuffle(buffer_size=10, reshuffle_each_iteration=True).batch(batch_size=batch_size, drop_remainder=True).repeat()\n","  iter = ds.make_one_shot_iterator()\n","  next = iter.get_next()\n","\n","  def usetrain():\n","    inputs = next[0]\n","    outputs = next[1]\n","    return inputs, outputs\n","  def usetest():\n","    return X_test, y_test\n","\n","  #Assert X_test is not None. This should be checked when running a session\n","  input, output = tf.cond(isTrain, usetrain, usetest)\n","  \n","  #Helper functions\n","  join = lambda l, u: tf.concat([l, u], axis=0) #Stack in the depth (batch, height, w, depth)\n","  labeled = lambda x: x[:num_labeled] if x is not None else x #Use tf.getitem (implicitly)\n","  unlabeled = lambda x: x[num_labeled:] if x is not None else x\n","  split_lu = lambda x: (labeled(x), unlabeled(x))\n","  \n","  #Running average for the clean pass and the labeled points\n","  ema = tf.train.ExponentialMovingAverage(decay=0.9999)  # to calculate the moving averages of mean and variance\n","  #Initialize with shapes (1,kernel_size, kernel_size, filter_size)\n","  running_mean = [tf.Variable(tf.constant(0.0, shape=[1,kernel_size,kernel_size,f]), trainable=False) for f in filter_size]\n","  running_mean = running_mean + [tf.Variable(tf.constant(0.0, shape=[N_CLASSES]))]\n","  running_var = [tf.Variable(tf.constant(1.0, shape=[1,kernel_size,kernel_size,f]), trainable=False) for f in filter_size]\n","  running_var = running_var + [tf.Variable(tf.constant(1.0, shape=[N_CLASSES]))]\n","  \n","  \n","  def new_activation_dict():\n","    return AttributeDict({'z': {}, 'h': {}, 's': {}, 'm': {}})\n"," \n","  W = tf.Variable(tf.random_normal(shape=[kernel_size**2 * filter_size[-1],N_CLASSES]))\n","  V = tf.Variable(tf.random_normal(shape=[N_CLASSES, kernel_size**2 * filter_size[-1]]))\n","  \n","  L = N+2 #Convs+In+Soft\n","  \n","  \n","  def g(z_lat, u):\n","    shape = tf.shape(u)[1:] #Don't take the batch size as a dimension \n","    wi = lambda inits, name: tf.Variable(inits * tf.ones(shape), name=name)\n","    a1 = wi(0., 'a1')\n","    a2 = wi(1., 'a2')\n","    a3 = wi(0., 'a3')\n","    a4 = wi(0., 'a4')\n","    a5 = wi(0., 'a5')\n","\n","    a6 = wi(0., 'a6')\n","    a7 = wi(1., 'a7')\n","    a8 = wi(0., 'a8')\n","    a9 = wi(0., 'a9')\n","    a10 = wi(0., 'a10')\n","\n","    mu = a1 * tf.sigmoid(a2 * u + a3) + a4 * u + a5\n","    v = a6 * tf.sigmoid(a7 * u + a8) + a9 * u + a10\n","\n","    z_est = (z_lat - mu) * v + mu\n","    return z_est\n","  \n","  #Encoder\n","  def encoder(input, noise_std):\n","    #Apply noise to the input\n","    h = tf.cast(input,tf.float32) + tf.random_normal(tf.shape(input)) * noise_std #Normal noise 0 mean 1 std\n","    d = AttributeDict() #This is what we will return. It will contain all the information we need\n","    d.unlabeled = new_activation_dict()\n","    d.labeled = new_activation_dict()\n","    d.unlabeled.z[0] = unlabeled(h)\n","    d.labeled.z[0] = labeled(h)\n","    \n","    for i in range(1,L): #Go through the convolutional layers, if we are at i==N+1, we need to flatten and apply W\n","      d.labeled.h[i-1], d.unlabeled.h[i-1] = split_lu(h)\n","      \n","      if i==L-1:\n","        z = tf.layers.flatten(h)\n","        z = tf.matmul(z,W)\n","      else:\n","        #Compute new z by applying convolution followed by ReLU after normalization\n","        z = tf.layers.conv2d(h,filters=filter_size[i-1], kernel_size=kernel_size, padding='same')\n","      #Shape: (?,5,5,filter_size)\n","      #Normalize\n","      z_lbld, z_unlbld = split_lu(z)\n","      \n","      keep_dims = True\n","      if i==L-1:\n","        keep_dims = False\n","        \n","      m_unlbld, s_unlbld = tf.nn.moments(z_unlbld, axes=[0], keep_dims=keep_dims) #Compute along depth\n","      m_lbld, s_lbld = tf.nn.moments(z_lbld, axes=[0], keep_dims=keep_dims)\n","      #Shape: (1,5,5,filter_size)\n","      \n","      if noise_std == 0: #Clean pass\n","        #Update the running averages and get the mean and variance of the labeled points again\n","        assign_mean = running_mean[i-1].assign(m_lbld)\n","        assign_var = running_var[i-1].assign(s_lbld)\n","        with tf.control_dependencies([assign_mean, assign_var]):\n","          ema.apply([running_mean[i-1], running_var[i-1]])\n","          m_lbld = ema.average(running_mean[i-1])\n","          s_lbld = ema.average(running_var[i-1])\n","\n","          \n","      z = join(\n","        (z_lbld-m_lbld) / tf.sqrt(s_lbld + 1e-10),\n","        (z_unlbld-m_unlbld) / tf.sqrt(s_unlbld + 1e-10))\n","      \n","      if noise_std > 0:\n","        z += tf.random_normal(tf.shape(z)) * noise_std\n","        \n","      z_lateral = z\n","      \n","      if i==L-1: #We need to apply softmax and multiply with gamma\n","        z += beta[i-1]\n","        z = tf.multiply(z, gamma)\n","        h = tf.nn.softmax(z)\n","      else:  \n","        #Now apply activation. But before we apply the activation, add beta and multiply\n","        #with gamma. Gamma is not used for ReLU. We apply Gamma for the softmax layer.\n","        z += beta[i-1] #i starts at 1, but beta starts at 0\n","        #Apply ReLU\n","        h = tf.nn.relu(z) #h gets assigned at the beginning of the for loop\n","      \n","      #Now save the variables: z_lateral, m_unlbld, s_unlbld, h\n","      d.labeled.z[i], d.unlabeled.z[i] = split_lu(z_lateral) #The real z has been compromised\n","      d.unlabeled.s[i] = s_unlbld\n","      d.unlabeled.m[i] = m_unlbld\n","      \n","    #Get the last h.\n","    d.labeled.h[i], d.unlabeled.h[i] = split_lu(h)\n","    \n","    return h, d\n","  #End encoder\n","  \n","  #Get the clean run\n","  y_clean, clean = encoder(input, noise_std=0.0)\n","  #Get the corrupted encoder run\n","  y_corrupted, corr = encoder(input, noise_std=noise_std)\n","  \n","  #Use this to store the z_est etc.\n","  est = new_activation_dict()\n","  \n","  #Decoder path\n","  filter_dims = [DEPTH] + filter_size\n","  #Start at index N+1 and go through index 0, N=3\n","  cost_recon = []\n","  for i in np.arange(L)[::-1]: #Start from L+1 --> 0, L+1 = N+2 = 6, 30-90-30-15-9\n","    #Get all the information we need\n","    z_corr = corr.unlabeled.z[i]\n","    z_clean = clean.unlabeled.z[i]\n","    \n","    #We don't have running mean and variance for the alreadz standardized input. TODO: Correct?\n","    if i != 0:\n","      z_clean_s = clean.unlabeled.s[i]\n","      z_clean_m = clean.unlabeled.m[i]\n","      \n","    if i==L-1: #The top level\n","      #Just normalize the (100,9) output\n","      ver = corr.unlabeled.h[i]\n","    elif i==L-2: #Apply the matrix V\n","      ver = tf.matmul(est.z.get(i+1), V) #This produces a (100,375)\n","      #Now reshape to [batch_size, kernel, kernel, filter[-1]]\n","      ver = tf.reshape(ver, shape=corr.unlabeled.z[N].shape)\n","    else:\n","      #Deconvolve. This is just a convolution to a new filter size. We leave the kernel size untouched.\n","      ver = tf.layers.conv2d(est.z.get(i+1),filters=filter_dims[i], kernel_size=kernel_size, padding='same')\n","  \n","    #Now normalize them\n","    keep_dims = True\n","    if i==L-1: #Only for the first dimension don't keep the dimension since it is (?,), not (?,5,5,?)\n","      keep_dims = False\n","        \n","    m, s = tf.nn.moments(ver, axes=[0], keep_dims=keep_dims) #Compute along depth\n","    ver = (ver-m) / tf.sqrt(s + 1e-10)\n","    \n","    #Now apply g to get z_est, g(z_corr_from_encoder, ver (u in the paper))\n","    z_est = g(z_corr, ver)\n","    \n","    #Now normalize using the clean mean and clean variance, but only if i != 0\n","    if i != 0:\n","      z_est_norm = (z_est - z_clean_m) / tf.sqrt(z_clean_s + 1e-10)\n","    else: z_est_norm = z_est\n","    \n","    #Now compute the cost and append the weighted cost\n","    c_tmp = tf.nn.l2_loss(z_clean-z_est_norm) * denoising_cost[i]\n","    cost_recon.append(c_tmp)\n","    est.z[i] = z_est_norm\n","    \n","  target = labeled(tf.one_hot(tf.cast(output,tf.int32),depth=N_CLASSES))\n","  \n","  y_corrupted = labeled(y_corrupted)\n","  supervised_cost = -tf.reduce_mean(tf.reduce_sum(target*tf.log(y_corrupted), 1))\n","  unsupervised_cost = tf.add_n(cost_recon)\n","  loss = supervised_cost + unsupervised_cost\n","  \n","  \n","  \n","  prediction_cost = -tf.reduce_mean(tf.reduce_sum(target*tf.log(labeled(y_clean)), 1),name='pred_cost')\n","  correct_prediction = tf.equal(tf.argmax(labeled(y_clean), 1), tf.argmax(target, 1), name='correct_prediction')\n","  accuracy = tf.multiply(tf.reduce_mean(tf.cast(correct_prediction, \"float\")),tf.constant(100.0),name='accuracy')\n","  \n","  learning_rate = tf.Variable(lr, trainable=False)\n","  train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","  \n","  \n","  debug = accuracy\n","\n","      \n","  with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    n_iter = int(N_EXAMPLES/batch_size)\n","    for epoch in range(num_epochs):\n","      for i in range(n_iter):\n","        _, acc = sess.run([train_step, accuracy], feed_dict={isTrain: True})\n","        #if i % 100 == 0: print(\"Training accuracy: %s\" % acc)\n","          \n","      acc = sess.run(accuracy, feed_dict={isTrain: True})\n","      print(\"Epoch No.: %s ACC: %s\" % (epoch, acc))\n","        \n","        \n","        \n","        \n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"O5AW9ymQ4_GV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1067},"outputId":"cd455056-663c-4948-820b-40ff820ee695","executionInfo":{"status":"error","timestamp":1539384043329,"user_tz":240,"elapsed":112606,"user":{"displayName":"julian buchel","photoUrl":"","userId":"03709653353653158720"}}},"cell_type":"code","source":["train(X_train,y_train,X_test,y_test,num_epochs=10)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Epoch No.: 0 ACC: 11.0\n","Epoch No.: 1 ACC: 17.0\n","Epoch No.: 2 ACC: 10.0\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-523feedc3532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-821f3e3c7c07>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, y, X_test, y_test, N, filter_size, kernel_size, denoising_cost, num_epochs, batch_size, num_labeled, noise_std, lr, decay_after)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0misTrain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;31m#if i % 100 == 0: print(\"Training accuracy: %s\" % acc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"lhWGVGbfWrST","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}