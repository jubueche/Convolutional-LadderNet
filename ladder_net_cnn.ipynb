{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ladder_net_cnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"woHpwj0ArpTy","colab_type":"text"},"cell_type":"markdown","source":["# Ladder Network with Convolutional Layers\n","**Input**: M,N,K specifying the general architecture: INPUT -> [[CONV -> RELU]*N -> POOL]*M -> [FC -> RELU]*K -> FC\n","\n","The Ladder network typically has a following layer structure: [Input size, say 700, 1000, 500, 250, 250, 250, 10]"]},{"metadata":{"id":"ix-ac8gfsbiY","colab_type":"text"},"cell_type":"markdown","source":["# Data"]},{"metadata":{"id":"1A_tCHSiroc0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":751},"outputId":"796780fe-0687-4835-94d8-4f9f919a5d97","executionInfo":{"status":"ok","timestamp":1541683158070,"user_tz":300,"elapsed":47282,"user":{"displayName":"julian buchel","photoUrl":"","userId":"03709653353653158720"}}},"cell_type":"code","source":["!pip install attributedict\n","import numpy as np\n","from sklearn.decomposition import PCA\n","import scipy.io as sio\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","import os\n","import random\n","from random import shuffle\n","from skimage.transform import rotate\n","import scipy.ndimage\n","from sklearn.model_selection import train_test_split\n","import scipy\n","\n","def load_pavia():\n","  \n","  !pip install GoogleDriveDownloader\n","  from google_drive_downloader import GoogleDriveDownloader as gdd\n","  gdd.download_file_from_google_drive(file_id='146WN2eZ6Syf-z1KMVRw9GmZdBu_g1JBj',\n","                                    dest_path='./datasets/paviau.mat', unzip=False)\n","\n","  gdd.download_file_from_google_drive(file_id='1L9OoAHnLVmPGbfKx8NhEbugxMzE1PG4j',\n","                                    dest_path='./datasets/paviau_gt.mat', unzip=False)\n","\n","  X = sio.loadmat('./datasets/paviau.mat')['paviaU']\n","  y = sio.loadmat('./datasets/paviau_gt.mat')['paviaU_gt']\n","\n","  return X, y\n","  \n","  \n","def createPatches(X, y, windowSize=5, removeZeroLabels = True):\n","  margin = int((windowSize - 1) / 2)\n","  zeroPaddedX = padWithZeros(X, margin=margin)\n","  # split patches\n","  patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n","  patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n","  patchIndex = 0\n","  for r in range(margin, zeroPaddedX.shape[0] - margin):\n","      for c in range(margin, zeroPaddedX.shape[1] - margin):\n","          patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n","          patchesData[patchIndex, :, :, :] = patch\n","          patchesLabels[patchIndex] = y[r-margin, c-margin]\n","          patchIndex = patchIndex + 1\n","  if removeZeroLabels:\n","      patchesData = patchesData[patchesLabels>0,:,:,:]\n","      patchesLabels = patchesLabels[patchesLabels>0]\n","      patchesLabels -= 1\n","  return patchesData, patchesLabels\n","  \n","  \n","def padWithZeros(X, margin=2):\n","  newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n","  x_offset = margin\n","  y_offset = margin\n","  newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n","  return newX\n","  \n","def standartizeData(X):\n","  newX = np.reshape(X, (-1, X.shape[2]))\n","  scaler = preprocessing.StandardScaler().fit(newX)  \n","  newX = scaler.transform(newX)\n","  newX = np.reshape(newX, (X.shape[0],X.shape[1],X.shape[2]))\n","  return newX, scaler\n","  \n","  \n","def applyPCA(X, numComponents=75):\n","  newX = np.reshape(X, (-1, X.shape[2]))\n","  pca = PCA(n_components=numComponents, whiten=True)\n","  newX = pca.fit_transform(newX)\n","  newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n","  return newX, pca\n","  \n","  \n","def diff(first, second):\n","  second = set(second)\n","  return [item for item in first if item not in second]\n","\n","\n","def splitTrainTestSet(X, y, testRatio=0.10):\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=345,\n","                                                      stratify=y)\n","  return X_train, X_test, y_train, y_test\n","  \n","  \n","def AugmentData(X_train):\n","  for i in range(int(X_train.shape[0]/2)):\n","      patch = X_train[i,:,:,:]\n","      num = random.randint(0,2)\n","      if (num == 0):\n","          flipped_patch = np.flipud(patch)\n","      if (num == 1):\n","          flipped_patch = np.fliplr(patch)\n","      if (num == 2):\n","          no = random.randrange(-180,180,30)\n","          flipped_patch = scipy.ndimage.interpolation.rotate(patch, no,axes=(1, 0),\n","                                                             reshape=False, output=None, order=3, mode='constant', cval=0.0, prefilter=False)\n","\n","      patch2 = flipped_patch\n","      X_train[i,:,:,:] = patch2\n","\n","  return X_train\n","  \n","  \n","def oversampleWeakClasses(X, y):\n","  uniqueLabels, labelCounts = np.unique(y, return_counts=True)\n","  maxCount = np.max(labelCounts)\n","  labelInverseRatios = maxCount / labelCounts  \n","  # repeat for every label and concat\n","  newX = X[y == uniqueLabels[0], :, :, :].repeat(round(labelInverseRatios[0]), axis=0)\n","  newY = y[y == uniqueLabels[0]].repeat(round(labelInverseRatios[0]), axis=0)\n","  for label, labelInverseRatio in zip(uniqueLabels[1:], labelInverseRatios[1:]):\n","      cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n","      cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n","      newX = np.concatenate((newX, cX))\n","      newY = np.concatenate((newY, cY))\n","  np.random.seed(seed=42)\n","  rand_perm = np.random.permutation(newY.shape[0])\n","  newX = newX[rand_perm, :, :, :]\n","  newY = newY[rand_perm]\n","  return newX, newY\n","  \n","  \n","  \n","def savePreprocessedData(X_trainPatches, X_testPatches, y_trainPatches, y_testPatches, windowSize):\n","  \n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","  \n","  with open(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/XtrainWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, X_trainPatches)\n","  with open(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/XtestWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, X_testPatches)\n","  with open(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/ytrainWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, y_trainPatches)\n","  with open(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/ytestWindowSize\" + str(windowSize) + \".npy\", 'wb') as outfile:\n","      np.save(outfile, y_testPatches)\n","      \n","      \n","      \n","      \n","# Global Variables\n","numComponents = 30\n","windowSize = 5\n","testRatio = 0.25\n","saved = True\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","if saved == False:\n","  X, y = load_pavia()\n","  X,_ = standartizeData(X)\n","  X,pca = applyPCA(X,numComponents=numComponents)\n","  XPatches, yPatches = createPatches(X, y, windowSize=windowSize)\n","  X_train, X_test, y_train, y_test = splitTrainTestSet(XPatches, yPatches, testRatio)\n","  print(X_train.shape)\n","  X_train, y_train = oversampleWeakClasses(X_train, y_train)\n","  X_train = AugmentData(X_train)\n","  savePreprocessedData(X_train, X_test, y_train, y_test, windowSize = windowSize)\n","  print(X_train.shape)\n","  \n","else:\n","  X_train = np.load(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/XtrainWindowSize\" + str(windowSize) + \".npy\")\n","  y_train = np.load(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/ytrainWindowSize\" + str(windowSize) + \".npy\")\n","  X_test = np.load(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/XtestWindowSize\" + str(windowSize) + \".npy\")\n","  y_test = np.load(\"/content/gdrive/My Drive/colab/Ladder-CNN/preprocessedData/ytestWindowSize\" + str(windowSize) + \".npy\")\n","  print(X_train.shape)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting attributedict\n","  Downloading https://files.pythonhosted.org/packages/f8/74/3b48e2749e1e96ba05fde101618de8f72dcb94cad78247677ee412fb8312/attributedict-0.1.8.tar.gz\n","Collecting easypackage>=0.1.8 (from attributedict)\n","  Downloading https://files.pythonhosted.org/packages/94/ae/858e97891b7b27f958a2fbc4fb42aacaa56f8c083e0e8043e713dea9bb5f/easypackage-0.1.8.tar.gz\n","Collecting deepdiff>=3.3.0 (from attributedict)\n","  Downloading https://files.pythonhosted.org/packages/50/0b/87df7f45ce7dc02aa576458ffdf146f0b350d541fce373a91e8a81751deb/deepdiff-3.3.0-py3-none-any.whl\n","Collecting tox>=3.0.0 (from attributedict)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/43/2160a300e0b77a929a980f36ac7427dcef8f4ddac7a8c21e5a8baedad828/tox-3.5.3-py2.py3-none-any.whl (53kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 6.8MB/s \n","\u001b[?25hCollecting pygments>=2.2.0 (from attributedict)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/ee/b6e02dc6529e82b75bb06823ff7d005b141037cb1416b10c6f00fc419dca/Pygments-2.2.0-py2.py3-none-any.whl (841kB)\n","\u001b[K    100% |████████████████████████████████| 849kB 7.2MB/s \n","\u001b[?25hCollecting colour-runner>=0.0.5 (from attributedict)\n","  Downloading https://files.pythonhosted.org/packages/d4/82/ce3250026add1910739dcabc796571ad1d182cb47332716c8bb96ee5d624/colour_runner-0.1.1-py2.py3-none-any.whl\n","Collecting jsonpickle (from deepdiff>=3.3.0->attributedict)\n","  Downloading https://files.pythonhosted.org/packages/ca/ce/97404d5aeb58e6155c216825c81b50f6eca8a5345c582317ae48391878f8/jsonpickle-1.0-py2.py3-none-any.whl\n","Requirement already satisfied: virtualenv>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (16.1.0)\n","Requirement already satisfied: setuptools>=30.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (40.5.0)\n","Requirement already satisfied: six<2,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (1.11.0)\n","Requirement already satisfied: pluggy<1,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (0.8.0)\n","Requirement already satisfied: py<2,>=1.4.17 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (1.7.0)\n","Requirement already satisfied: filelock<4,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (3.0.10)\n","Requirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.6/dist-packages (from tox>=3.0.0->attributedict) (0.10.0)\n","Collecting blessings (from colour-runner>=0.0.5->attributedict)\n","  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n","Building wheels for collected packages: attributedict, easypackage\n","  Running setup.py bdist_wheel for attributedict ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ff/8f/74/0b6d96ba8ce4389b1f6418c9c928c1c3e4c4e8f510242fa0a6\n","  Running setup.py bdist_wheel for easypackage ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f5/a9/98/2fd45c109d32235245fe4be30e4826b3d6c9aa84d6020abbb5\n","Successfully built attributedict easypackage\n","Installing collected packages: easypackage, jsonpickle, deepdiff, tox, pygments, blessings, colour-runner, attributedict\n","  Found existing installation: Pygments 2.1.3\n","    Uninstalling Pygments-2.1.3:\n","      Successfully uninstalled Pygments-2.1.3\n","Successfully installed attributedict-0.1.8 blessings-1.7 colour-runner-0.1.1 deepdiff-3.3.0 easypackage-0.1.8 jsonpickle-1.0 pygments-2.2.0 tox-3.5.3\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","(128051, 5, 5, 30)\n"],"name":"stdout"}]},{"metadata":{"id":"CYmNha5nTMiK","colab_type":"text"},"cell_type":"markdown","source":["# CIFAR-10 Data\n","from [https://github.com/wenxinxu/resnet-in-tensorflow]\n"]},{"metadata":{"id":"_h6-JwZbTQVc","colab_type":"code","colab":{}},"cell_type":"code","source":["import tarfile\n","from six.moves import urllib\n","import sys\n","import numpy as np\n","import pickle\n","import os\n","import cv2\n","import tensorflow as tf\n","\n","data_dir = 'cifar10_data'\n","full_data_dir = 'cifar10_data/cifar-10-batches-py/data_batch_'\n","vali_dir = 'cifar10_data/cifar-10-batches-py/test_batch'\n","DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n","\n","\n","IMG_WIDTH = 32\n","IMG_HEIGHT = 32\n","IMG_DEPTH = 3\n","NUM_CLASS = 10\n","\n","TRAIN_RANDOM_LABEL = False # Want to use random label for train data?\n","VALI_RANDOM_LABEL = False # Want to use random label for validation?\n","\n","NUM_TRAIN_BATCH = 5 # How many batches of files you want to read in, from 0 to 5)\n","EPOCH_SIZE = 10000 * NUM_TRAIN_BATCH\n","\n","\n","def maybe_download_and_extract():\n","    '''\n","    Will download and extract the cifar10 data automatically\n","    :return: nothing\n","    '''\n","    dest_directory = data_dir\n","    if not os.path.exists(dest_directory):\n","        os.makedirs(dest_directory)\n","    filename = DATA_URL.split('/')[-1]\n","    filepath = os.path.join(dest_directory, filename)\n","    if not os.path.exists(filepath):\n","        def _progress(count, block_size, total_size):\n","            sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename, float(count * block_size)\n","                                                             / float(total_size) * 100.0))\n","            sys.stdout.flush()\n","        filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n","        print()\n","        statinfo = os.stat(filepath)\n","        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n","        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n","\n","\n","def _read_one_batch(path, is_random_label):\n","    '''\n","    The training data contains five data batches in total. The validation data has only one\n","    batch. This function takes the directory of one batch of data and returns the images and\n","    corresponding labels as numpy arrays\n","    :param path: the directory of one batch of data\n","    :param is_random_label: do you want to use random labels?\n","    :return: image numpy arrays and label numpy arrays\n","    '''\n","    fo = open(path, 'rb')\n","    dicts = pickle.load(fo, encoding='latin1')\n","    fo.close()\n","\n","    data = dicts['data']\n","    if is_random_label is False:\n","        label = np.array(dicts['labels'])\n","    else:\n","        labels = np.random.randint(low=0, high=10, size=10000)\n","        label = np.array(labels)\n","    return data, label\n","\n","\n","def read_in_all_images(address_list, shuffle=True, is_random_label = False):\n","    \"\"\"\n","    This function reads all training or validation data, shuffles them if needed, and returns the\n","    images and the corresponding labels as numpy arrays\n","    :param address_list: a list of paths of cPickle files\n","    :return: concatenated numpy array of data and labels. Data are in 4D arrays: [num_images,\n","    image_height, image_width, image_depth] and labels are in 1D arrays: [num_images]\n","    \"\"\"\n","    data = np.array([]).reshape([0, IMG_WIDTH * IMG_HEIGHT * IMG_DEPTH])\n","    label = np.array([])\n","\n","    for address in address_list:\n","        print('Reading images from ' + address)\n","        batch_data, batch_label = _read_one_batch(address, is_random_label)\n","        # Concatenate along axis 0 by default\n","        data = np.concatenate((data, batch_data))\n","        label = np.concatenate((label, batch_label))\n","\n","    num_data = len(label)\n","\n","    # This reshape order is really important. Don't change\n","    # Reshape is correct. Double checked\n","    data = data.reshape((num_data, IMG_HEIGHT * IMG_WIDTH, IMG_DEPTH), order='F')\n","    data = data.reshape((num_data, IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH))\n","\n","\n","    if shuffle is True:\n","        print('Shuffling')\n","        order = np.random.permutation(num_data)\n","        data = data[order, ...]\n","        label = label[order]\n","\n","    data = data.astype(np.float32)\n","    return data, label\n","\n","\n","def horizontal_flip(image, axis):\n","    '''\n","    Flip an image at 50% possibility\n","    :param image: a 3 dimensional numpy array representing an image\n","    :param axis: 0 for vertical flip and 1 for horizontal flip\n","    :return: 3D image after flip\n","    '''\n","    flip_prop = np.random.randint(low=0, high=2)\n","    if flip_prop == 0:\n","        image = cv2.flip(image, axis)\n","\n","    return image\n","\n","\n","def whitening_image(image_np):\n","    '''\n","    Performs per_image_whitening\n","    :param image_np: a 4D numpy array representing a batch of images\n","    :return: the image numpy array after whitened\n","    '''\n","    for i in range(len(image_np)):\n","        mean = np.mean(image_np[i, ...])\n","        # Use adjusted standard deviation here, in case the std == 0.\n","        std = np.max([np.std(image_np[i, ...]), 1.0/np.sqrt(IMG_HEIGHT * IMG_WIDTH * IMG_DEPTH)])\n","        image_np[i,...] = (image_np[i, ...] - mean) / std\n","    return image_np\n","\n","\n","def random_crop_and_flip(batch_data, padding_size):\n","    '''\n","    Helper to random crop and random flip a batch of images\n","    :param padding_size: int. how many layers of 0 padding was added to each side\n","    :param batch_data: a 4D batch array\n","    :return: randomly cropped and flipped image\n","    '''\n","    cropped_batch = np.zeros(len(batch_data) * IMG_HEIGHT * IMG_WIDTH * IMG_DEPTH).reshape(\n","        len(batch_data), IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH)\n","\n","    for i in range(len(batch_data)):\n","        x_offset = np.random.randint(low=0, high=2 * padding_size, size=1)[0]\n","        y_offset = np.random.randint(low=0, high=2 * padding_size, size=1)[0]\n","        cropped_batch[i, ...] = batch_data[i, ...][x_offset:x_offset+IMG_HEIGHT,\n","                      y_offset:y_offset+IMG_WIDTH, :]\n","\n","        cropped_batch[i, ...] = horizontal_flip(image=cropped_batch[i, ...], axis=1)\n","\n","    return cropped_batch\n","\n","\n","def prepare_train_data(padding_size):\n","    '''\n","    Read all the train data into numpy array and add padding_size of 0 paddings on each side of the\n","    image\n","    :param padding_size: int. how many layers of zero pads to add on each side?\n","    :return: all the train data and corresponding labels\n","    '''\n","    path_list = []\n","    for i in range(1, NUM_TRAIN_BATCH+1):\n","        path_list.append(full_data_dir + str(i))\n","    data, label = read_in_all_images(path_list, is_random_label=TRAIN_RANDOM_LABEL)\n","    \n","    pad_width = ((0, 0), (padding_size, padding_size), (padding_size, padding_size), (0, 0))\n","    data = np.pad(data, pad_width=pad_width, mode='constant', constant_values=0)\n","    \n","    return data, label\n","\n","\n","def read_validation_data():\n","    '''\n","    Read in validation data. Whitening at the same time\n","    :return: Validation image data as 4D numpy array. Validation labels as 1D numpy array\n","    '''\n","    validation_array, validation_labels = read_in_all_images([vali_dir],\n","                                                       is_random_label=VALI_RANDOM_LABEL)\n","    validation_array = whitening_image(validation_array)\n","\n","    return validation_array, validation_labels"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KqgVmBZXTa3n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"a2591cf7-ca02-4d0f-fbc5-cb4a9e3ac9ac","executionInfo":{"status":"ok","timestamp":1541378821865,"user_tz":300,"elapsed":4635,"user":{"displayName":"julian buchel","photoUrl":"","userId":"03709653353653158720"}}},"cell_type":"code","source":["maybe_download_and_extract()\n","X,y = prepare_train_data(padding_size = 0)\n","X_val, y_val = read_validation_data()\n","\n","print(X.shape)\n","print(X_val.shape)"],"execution_count":91,"outputs":[{"output_type":"stream","text":["Reading images from cifar10_data/cifar-10-batches-py/data_batch_1\n","Reading images from cifar10_data/cifar-10-batches-py/data_batch_2\n","Reading images from cifar10_data/cifar-10-batches-py/data_batch_3\n","Reading images from cifar10_data/cifar-10-batches-py/data_batch_4\n","Reading images from cifar10_data/cifar-10-batches-py/data_batch_5\n","Shuffling\n","Reading images from cifar10_data/cifar-10-batches-py/test_batch\n","Shuffling\n","(50000, 32, 32, 3)\n","(10000, 32, 32, 3)\n"],"name":"stdout"}]},{"metadata":{"id":"IBRmrQQUy6D6","colab_type":"text"},"cell_type":"markdown","source":["# Conv. Ladder Net\n","Architecture: INPUT -> [[CONV -> RELU]*N -> POOL]*M -> [FC -> RELU]*K -> FC\n","\n","**Params**: N,M,K,filter_size (Array of length N)\n","\n","**Default**: N=3,M=0,K=1, filter_size=[3*PCA_comp=90, 30, 15]\n","\n","**For now use only convolution**\n"]},{"metadata":{"id":"FGFqQIyxsOYM","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","from attributedict.collections import AttributeDict\n","\n","def train(X,y,X_test=None,y_test=None,N=3,filter_size=[90,30,15],fc=[],kernel_size=5,\n","          denoising_cost=[10,1,0.1,0.1,0.1],num_epochs=150,batch_size=200,num_labeled=100,noise_std=0.3,lr=0.02,\n","          decay_after=15):\n","  \n","  assert len(denoising_cost) is 2+len(filter_size)+len(fc), \"Please specify denoising cost for every Layer. len(denoising_cost) != 2+len(fc)+len(filter_size)\"\n","  \n","  tf.reset_default_graph()\n","  tf.set_random_seed(12345)\n","  #We double the batch size here. This has the advantage that in case num_labeled is -1 (use all labels) we can use half of the\n","  #batch size for the clean encoder and the other half for the unsupervised run\n","  batch_size *= 2\n"," \n","  #Number of convolutions\n","  N = len(filter_size)\n","  #Number of fully connected layers\n","  K = len(fc)\n","  \n","  #Shape of X: (?,WND_SZE,WND_SZE, N_CHANNELS)\n","  WND_SZE = X.shape[1]\n","  N_CHANNELS = X.shape[3]\n","  N_CLASSES = len(np.unique(y))\n","  N_EXAMPLES = X.shape[0]\n","  DEPTH = X.shape[-1]\n","  \n","  L = K+N+2 #Input+Convs+Softmax\n","  \n","  #Create list of action,output-shape pairs, e.g. fs=[90,30,15] & fc=[100,50,20] would correspond to\n","  #{'conv',(?,5,5,90);'conv',(?,5,5,30);'conv',(?,5,5,15);'relu',(?,100);'relu',(?,50);'relu',(?,20)}\n","  #Implicit: 'flatten' & 'softmax'\n","  shapes = [('conv',s) for s in filter_size]+[('relu',s) for s in fc]+[('softmax',N_CLASSES)]\n","  \n","  num_labeled_tf = tf.placeholder(tf.int32, shape=())\n","  \n","  n_classes = len(np.unique(y_test))\n","  n_labeled_per_class = int(num_labeled/n_classes) #22\n","\n","  #Create X_labeled and X_unlabeled where X_labeled has num_labeled entries which are balanced w.r.t. the class labels\n","  indices = np.arange(len(y))\n","  i_labeled = []\n","  for c in range(n_classes):\n","        i = indices[y==c][:n_labeled_per_class]\n","        i_labeled += list(i)\n","\n","\n","  X_labeled = X[i_labeled,:,:,:]\n","  y_labeled = y[i_labeled]\n","\n","\n","  if num_labeled > batch_size:\n","    n_labeled_per_class = int(0.5*batch_size/n_classes) #Use 100 points for the unlabeled and the rest for the labeled\n","  else:\n","    n_labeled_per_class = int(num_labeled/n_classes)\n","\n","  #Take everything as unlabeled data\n","  X_unlabeled = X\n","\n","  #Create dataset from tensor slices\n","  features_placeholder_labeled = tf.placeholder(X_labeled.dtype, X_labeled.shape)\n","  features_placeholder = tf.placeholder(X_unlabeled.dtype, X_unlabeled.shape)\n","  labels_placeholder = tf.placeholder(y.dtype, y.shape) #This is for num_labeled == -1\n","  labels_placeholder_labeled = tf.placeholder(y_labeled.dtype, y_labeled.shape)\n","\n","  ds_lab = tf.data.Dataset.from_tensor_slices((features_placeholder_labeled, labels_placeholder_labeled))\n","\n","  ds_unlab = tf.data.Dataset.from_tensor_slices(features_placeholder)\n","  ds_unlab = ds_unlab.shuffle(buffer_size=10, reshuffle_each_iteration=True).batch(batch_size=batch_size-n_labeled_per_class*n_classes, drop_remainder=True).repeat()\n","  print(\"Size unlab batch: %s\" % (batch_size-n_labeled_per_class*n_classes))\n","\n","  ds_full = tf.data.Dataset.from_tensor_slices((features_placeholder,labels_placeholder))\n","  ds_full = ds_full.shuffle(buffer_size=10, reshuffle_each_iteration=True).batch(batch_size=batch_size, drop_remainder=True).repeat()\n","\n","  iterator_full = ds_full.make_initializable_iterator()\n","  iterator_unlab = ds_unlab.make_initializable_iterator()\n","  #Create datasets for each class\n","  datasets = [ds_lab.filter(lambda x,y : tf.equal(y,lab)) for lab in range(n_classes)]\n","  iterators = []\n","  nexts = []\n","\n","  next = ()\n","\n","  if num_labeled != -1:\n","    for idx,d in enumerate(datasets):\n","      datasets[idx] = d.shuffle(buffer_size=10, reshuffle_each_iteration=True).batch(batch_size=n_labeled_per_class, drop_remainder=True).repeat()\n","      iterators =iterators + [datasets[idx].make_initializable_iterator()]\n","      nexts = nexts + [iterators[idx].get_next()]\n","\n","    seed = np.random.randint(100)\n","    X_out = tf.random.shuffle(tf.concat([x[0] for x in nexts],axis=0),seed = seed)\n","    y_out = tf.random.shuffle(tf.concat([x[1] for x in nexts],axis=0),seed = seed)\n","    y_out_again = tf.random.shuffle(tf.concat([x[1] for x in nexts],axis=0),seed = seed)\n","\n","    X_out_un = iterator_unlab.get_next()\n","    y_out_un = tf.constant(shape=([batch_size-n_labeled_per_class*n_classes]), value = -1,dtype=tf.float64)\n","\n","    next = (tf.concat([X_out,X_out_un],axis=0), tf.concat([y_out,y_out_un],axis=0))\n","    print(next)\n","  else:\n","    next = iterator_full.get_next()\n","\n","  \n","  if num_labeled == -1 or num_labeled > batch_size:\n","    num_labeled = batch_size/2 #Since we doubled the batch size before.\n","    \n","  \n","  features_placeholder_test = tf.placeholder(X_test.dtype, shape=(None,WND_SZE,WND_SZE,N_CHANNELS),name='X_test')\n","  labels_placeholder_test = tf.placeholder(y_test.dtype, shape=(None,),name='y_test')\n","  \n","  \n","  inputs =  tf.placeholder(tf.float32, shape=(None,WND_SZE,WND_SZE,N_CHANNELS),name='inputs')\n","  outputs = tf.placeholder(tf.float32, shape=(None,),name='outputs')\n","  isTrain = tf.placeholder(tf.bool, shape=())\n","  \n","  \n","  #Gamma and beta initialization: Need one gamma (for softmax) and N+K many with different shapes. \n","  gamma = tf.Variable(tf.ones([N_CLASSES])) #Take the prev. to last one e.g. 90\n","  beta = [tf.Variable(tf.zeros([kernel_size,kernel_size,fs])) for fs in filter_size]+[tf.Variable(tf.zeros([s])) for s in fc]\n","  beta = beta + [tf.Variable(tf.zeros([N_CLASSES]))] #For the last layer\n","  \n","  def usetrain():\n","    inputs = next[0]\n","    outputs = next[1]\n","    return inputs, outputs\n","  def usetest():\n","    return features_placeholder_test, labels_placeholder_test\n","\n","  assert X_test is not None, \"Check if Test data is present in session\"\n","  input, output = tf.cond(isTrain, usetrain, usetest)\n","  \n","  #Helper functions\n","  join = lambda l, u: tf.concat([l, u], axis=0) #Stack in the depth (batch, height, w, depth)\n","  labeled = lambda x: x[:num_labeled_tf] if x is not None else x #Use tf.getitem (implicitly)\n","  unlabeled = lambda x: x[num_labeled_tf:] if x is not None else x\n","  split_lu = lambda x: (labeled(x), unlabeled(x))\n","  \n","  #Running average for the clean pass and the labeled points\n","  ema = tf.train.ExponentialMovingAverage(decay=0.9999)  # to calculate the moving averages of mean and variance\n","  bn_assigns = []\n","  #Initialize with shapes (1,kernel_size, kernel_size, filter_size)\n","  running_mean = [tf.Variable(tf.constant(0.0, shape=[1,kernel_size,kernel_size,f]), trainable=False) for f in filter_size]+[tf.Variable(tf.constant(0.0, shape=[s]), trainable=False) for s in fc]\n","  running_mean = running_mean + [tf.Variable(tf.constant(0.0, shape=[N_CLASSES]))]\n","  running_var = [tf.Variable(tf.constant(1.0, shape=[1,kernel_size,kernel_size,f]), trainable=False) for f in filter_size]+[tf.Variable(tf.constant(1.0, shape=[s]), trainable=False) for s in fc]\n","  running_var = running_var + [tf.Variable(tf.constant(1.0, shape=[N_CLASSES]))]\n","  \n","  \n","  def new_activation_dict():\n","    return AttributeDict({'z': {}, 'h': {}, 's': {}, 'm': {}})\n","  \n","  if shapes[-2][0] == 'conv':\n","    W = tf.Variable(tf.random_normal(shape=[kernel_size**2 * filter_size[-1],N_CLASSES])) #In case the last layer is a conv layer\n","    V = tf.Variable(tf.random_normal(shape=[N_CLASSES, kernel_size**2 * filter_size[-1]])) #Matrix for decoder. Takes the softmax layer shape (?,9) -> (?,kernel_size**2 * filter_size[-1]) to then reshape to a tensor  \n","  else:\n","    W = tf.Variable(tf.random_normal(shape=[shapes[-2][1],N_CLASSES])) #In case the last layer is a fully connected layer.\n","    V = tf.Variable(tf.random_normal(shape=[N_CLASSES,shapes[-2][1]])) #Matrix for decoder. Takes the softmax layer shape (?,9) -> (?,kernel_size**2 * filter_size[-1]) / (?,fully_connected_shape) -> reshape or not\n","  \n","  if K>0:\n","    W_fc = [tf.Variable(tf.random_normal(shape=[kernel_size**2 * filter_size[-1],fc[0]]))] #The first weight matrix for the fc part.\n","    V_fc = [tf.Variable(tf.random_normal(shape=[fc[0],kernel_size**2 * filter_size[-1]]))] #Input dimesnion is fc[0], e.g. fc=[10,20,30], encoder: (?,5,5,30)->(?,10)->(?,20)->(?,30)->(?,9)->(Decoder)(?,30)->(?,20)->(?,10)->(?,5,5,30)\n","    if K>1: #TODO: Works without if? \n","      W_fc = W_fc + [tf.Variable(tf.random_normal(shape=[fc[i-1],fc[i]])) for i in range(1,K)]\n","      V_fc = V_fc + [tf.Variable(tf.random_normal(shape=[fc[i],fc[i-1]])) for i in range(1,K)] #The matrix that \n","      print(V_fc)\n","\n","  \n","  def g(z_lat, u, size):\n","    shape = tf.shape(u)[1:] #Don't take the batch size as a dimension \n","    wi = lambda inits, name: tf.Variable(inits * tf.ones(size), name=name)\n","    a1 = wi(0., 'a1')\n","    a2 = wi(1., 'a2')\n","    a3 = wi(0., 'a3')\n","    a4 = wi(0., 'a4')\n","    a5 = wi(0., 'a5')\n","    a6 = wi(0., 'a6')\n","    a7 = wi(1., 'a7')\n","    a8 = wi(0., 'a8')\n","    a9 = wi(0., 'a9')\n","    a10 = wi(0., 'a10')\n","    mu = a1 * tf.sigmoid(a2 * u + a3) + a4 * u + a5\n","    v = a6 * tf.sigmoid(a7 * u + a8) + a9 * u + a10\n","    z_est = (z_lat - mu) * v + mu\n","    return z_est\n","  \n","  \n","  #Encoder\n","  def encoder(input, noise_std):\n","    with tf.variable_scope('encoder', reuse=tf.AUTO_REUSE):\n","      #Apply noise to the input\n","      h = tf.cast(input,tf.float32) + tf.random_normal(dtype=tf.float32,shape=tf.shape(input)) * noise_std #Normal noise 0 mean 1 std\n","      d = AttributeDict() #This is what we will return. It will contain all the information we need\n","      d.unlabeled = new_activation_dict()\n","      d.labeled = new_activation_dict()\n","      d.unlabeled.z[0] = unlabeled(h)\n","      d.labeled.z[0] = labeled(h)\n","\n","      for i in range(1,L): #Go through the convolutional layers, if we are at i==N+1, we need to flatten and apply W\n","        d.labeled.h[i-1], d.unlabeled.h[i-1] = split_lu(h)\n","        \n","        operation = shapes[i-1][0]\n","        output_shape = shapes[i-1][1]\n","\n","        if operation=='softmax':\n","          z = tf.layers.flatten(h)\n","          z = tf.matmul(z,W)\n","          keep_dims = False\n","        elif operation=='conv':\n","          #Compute new z by applying convolution followed by ReLU after normalization\n","          z = tf.layers.conv2d(h,filters=filter_size[i-1], kernel_size=kernel_size, padding='same')\n","          keep_dims = True\n","        else:\n","          #No need to check for input dim, because flatten preserves batch axis\n","          z = tf.layers.flatten(h)\n","          z = tf.matmul(z,W_fc[i-1-N])\n","          keep_dims = False\n","          \n","        #Shape: (?,5,5,filter_size) or (?,fc_size)\n","        #Normalize\n","        z_lbld, z_unlbld = split_lu(z)\n","\n","        m_unlbld, s_unlbld = tf.nn.moments(z_unlbld, axes=[0], keep_dims=keep_dims) #Compute along depth\n","        m_lbld, s_lbld = tf.nn.moments(z_lbld, axes=[0], keep_dims=keep_dims)\n","        #Shape: (1,5,5,filter_size)\n","\n","        if noise_std == 0: #Clean pass\n","          #Update the running averages and get the mean and variance of the labeled points again\n","          assign_mean = running_mean[i-1].assign(m_lbld)\n","          assign_var = running_var[i-1].assign(s_lbld)\n","          with tf.control_dependencies([assign_mean, assign_var]):\n","            bn_assigns.append(ema.apply([running_mean[i-1], running_var[i-1]]))\n","            m_lbld = ema.average(running_mean[i-1])\n","            s_lbld = ema.average(running_var[i-1])\n","\n","\n","        z = join(\n","          (z_lbld-m_lbld) / tf.sqrt(s_lbld + 1e-10),\n","          (z_unlbld-m_unlbld) / tf.sqrt(s_unlbld + 1e-10))\n","\n","        if noise_std > 0:\n","          z += tf.random_normal(tf.shape(z)) * noise_std\n","\n","        z_lateral = z\n","\n","        if i==L-1: #We need to apply softmax and multiply with gamma\n","          z = tf.add(z,beta[i-1])\n","          z = tf.multiply(z, gamma)\n","          h = tf.nn.softmax(z)\n","        else:  \n","          #Now apply activation. But before we apply the activation, add beta and multiply\n","          #with gamma. Gamma is not used for ReLU. We apply Gamma for the softmax layer.\n","          z += beta[i-1] #i starts at 1, but beta starts at 0\n","          #Apply ReLU\n","          h = tf.nn.relu(z) #h gets assigned at the beginning of the for loop\n","\n","        #Now save the variables: z_lateral, m_unlbld, s_unlbld, h\n","        d.labeled.z[i], d.unlabeled.z[i] = split_lu(z_lateral) #The real z has been compromised\n","        d.unlabeled.s[i] = s_unlbld\n","        d.unlabeled.m[i] = m_unlbld\n","\n","      #Get the last h.\n","      d.labeled.h[i], d.unlabeled.h[i] = split_lu(h)\n","\n","      return h, d\n","   #End encoder\n","  \n","  \n","  #If isTrain is false, use the encoder without the splitting\n","  y_clean, clean = encoder(input, noise_std=0.0)\n","  \n","  #Get the clean run\n","  #y_clean, clean = encoder(input, noise_std=0.0, isTrain=True)\n","  #Get the corrupted encoder run\n","  y_corrupted, corr = encoder(input, noise_std=noise_std)\n","  \n","  #Use this to store the z_est etc.\n","  est = new_activation_dict()\n","  \n","  #Decoder path\n","  filter_dims = [DEPTH] + filter_size\n","  #Start at index N+1 and go through index 0, N=3\n","  cost_recon = []\n","  for i in np.arange(L)[::-1]: #Start from L-1 --> 0, L+1 = N+2 = 6, 30-90-30-15-9\n","    #Get all the information we need\n","    z_corr = corr.unlabeled.z[i]\n","    z_clean = clean.unlabeled.z[i]\n","    if i != 0:\n","      z_clean_s = clean.unlabeled.s[i]\n","      z_clean_m = clean.unlabeled.m[i]\n","      \n","    if i==L-1: #The top level\n","      #Just normalize the (?,9) output\n","      ver = corr.unlabeled.h[i]\n","      size = [N_CLASSES]\n","      keep_dims = False\n","    elif i==L-2: #Apply the matrix V\n","      ver = tf.matmul(est.z.get(i+1), V) #This produces a (?,375)\n","      if K==0: #If we do not have any fully connected layers after this, then reshape\n","        ver = tf.reshape(ver, shape=[-1,WND_SZE,WND_SZE,filter_size[-1]])\n","        size = [WND_SZE, WND_SZE, filter_size[-1]]\n","        keep_dims = True\n","      else:\n","        size = [fc[-1]]\n","        keep_dims = False\n","    else:\n","      #Get the corresponding operation:\n","      operation = shapes[i][0]\n","      print(operation)\n","      if operation == 'conv':\n","        #Deconvolve. This is just a convolution to a new filter size. We leave the kernel size untouched.\n","        ver = tf.layers.conv2d(est.z.get(i+1),filters=filter_dims[i], kernel_size=kernel_size, padding='same')\n","        size = [WND_SZE, WND_SZE, filter_dims[i]]\n","        keep_dims = True\n","      else: #Operation must be to apply the V_fc matrix\n","        tmp = tf.layers.flatten(est.z.get(i+1)) #Flatten. Note: This can bet optimized by checking if we really need to reshape\n","        print(tmp)\n","        print(V_fc[i-N])\n","        ver = tf.matmul(tmp,V_fc[i-N])\n","        if (i-N) == 0: #This was the last fully connected layer, now reshape\n","          ver = tf.reshape(ver, shape=[-1,WND_SZE,WND_SZE,filter_size[-1]])\n","          size = [WND_SZE, WND_SZE, filter_size[-1]]\n","          keep_dims = True\n","        else:\n","          size = [fc[i-1-N]]\n","          keep_dims = False\n","        \n","    print(size)\n","    m, s = tf.nn.moments(ver, axes=[0], keep_dims=keep_dims) #Compute along depth\n","    ver = (ver-m) / tf.sqrt(s + 1e-10)\n","    \n","    #Now apply g to get z_est, g(z_corr_from_encoder, ver (u in the paper))\n","    z_est = g(z_corr, ver, size)\n","    \n","    #Now normalize using the clean mean and clean variance, but only if i != 0\n","    if i != 0:\n","      z_est_norm = (z_est - z_clean_m) / tf.sqrt(z_clean_s + 1e-10)\n","    else:\n","      z_est_norm = z_est\n","    \n","    #Now compute the cost and append the weighted cost. Weigh by the size of the layer and the denoising cost\n","    c_tmp = (tf.reduce_mean(tf.reduce_sum(tf.square(z_est_norm - z_clean), 1)) / tf.cast(tf.reduce_prod(tf.shape(z_est)[1:]),dtype=tf.float32)) * denoising_cost[i]\n","    cost_recon.append(c_tmp)\n","    est.z[i] = z_est_norm  \n","  \n","    \n","  y_corrupted = labeled(y_corrupted)\n","  target = labeled(tf.one_hot(tf.cast(output,tf.int32),depth=N_CLASSES))\n","  target = tf.cast(target, dtype=tf.float32)\n","  yy = labeled(y_clean)  \n","    \n","  with tf.name_scope('supervised_cost'):\n","    supervised_cost = -tf.reduce_mean(tf.reduce_sum(target*tf.log(y_corrupted), 1), name='supervised_cost')\n","  supervised_cost_sum = tf.summary.scalar('supervised_cost', supervised_cost)\n","    \n","  with tf.name_scope('unsupervised_cost'):\n","    #unsupervised_cost = tf.add_n(cost_recon, name='unsupervised_cost')\n","    unsupervised_cost = tf.cond(isTrain, lambda: tf.add_n(cost_recon, name='unsupervised_cost'), lambda: tf.constant(0,dtype=tf.float32, shape=()))\n","  tf.summary.scalar('unsupervised_cost', unsupervised_cost)\n","  \n","  with tf.name_scope('total'):  \n","    loss = supervised_cost + unsupervised_cost\n","  tf.summary.scalar('total', loss)\n","\n","   \n","  prediction_cost = -tf.reduce_mean(tf.reduce_sum(target*tf.log(yy), 1),name='pred_cost')\n","  correct_prediction = tf.equal(tf.argmax(yy,1), tf.argmax(target, 1), name='correct_prediction')\n","  with tf.name_scope('accuracy'):\n","    accuracy = tf.multiply(tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32)),tf.constant(100.0),name='accuracy')\n","  accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n","  \n","  \n","  learning_rate = tf.Variable(lr, trainable=False)\n","  train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","  \n","  # add the updates of batch normalization statistics to train_step\n","  bn_updates = tf.group(*bn_assigns)\n","  with tf.control_dependencies([train_step]):\n","     train_step = tf.group(bn_updates)\n","  \n","  saver = tf.train.Saver()\n","  sess = tf.Session()\n","  \n","  merged = tf.summary.merge_all()\n","  merged2 = tf.summary.merge([supervised_cost_sum, accuracy_sum])\n","  train_writer = tf.summary.FileWriter('./log_ladder/train' , sess.graph)\n","  test_writer = tf.summary.FileWriter('./log_ladder/test')\n","  \n","  sess.run(tf.global_variables_initializer())\n","\n","  \n","  #Initialize the iterators for the data\n","  sess.run(iterator_full.initializer, feed_dict={features_placeholder: X, labels_placeholder:y })\n","\n","  sess.run(iterator_unlab.initializer, feed_dict={features_placeholder: X})\n","  for iterator in iterators:\n","    sess.run(iterator.initializer, feed_dict={features_placeholder_labeled: X_labeled,\n","                                            labels_placeholder_labeled: y_labeled})\n","  \n","  \n","  #Restore checkpoints, if any\n","  i_iter = 0\n","  ckpt = tf.train.get_checkpoint_state('checkpoints/')\n","  if ckpt and ckpt.model_checkpoint_path:\n","    print(\"Found checkpont! Restore...\")\n","    saver.restore(sess, ckpt.model_checkpoint_path)\n","    epoch_n = int(ckpt.model_checkpoint_path.split('-')[1])\n","    i_iter = epoch_n+1\n","    print(\"Restored Epoch %s\" % epoch_n)\n","  else:\n","    print(\"No checkpoint, initialize variables...\")\n","    if not os.path.exists('checkpoints'):\n","      os.makedirs('checkpoints')\n","    sess.run(tf.global_variables_initializer()) # initialization\n","    print(sess.run(tf.report_uninitialized_variables()))\n","  \n","  \n","  \n","  def train_acc():\n","    acc = sess.run(accuracy, feed_dict={isTrain: False, num_labeled_tf:X_train.shape[0], features_placeholder_test: X_train, labels_placeholder_test:y_train})\n","    return acc\n","  \n","  def test_acc():\n","    acc = sess.run(accuracy, feed_dict={isTrain: False, num_labeled_tf:X_test.shape[0], features_placeholder_test: X_test, labels_placeholder_test:y_test})\n","    return acc\n","  \n","  \n","  n_iter = int(N_EXAMPLES/batch_size)\n","  for epoch in range(i_iter,num_epochs):\n","    for i in range(n_iter):\n","      \n","      #Training step. Set num_labeled to the true num_labeled so that we split the data accordingly.\n","      sess.run(train_step, feed_dict={isTrain: True, num_labeled_tf:num_labeled, features_placeholder_test: X_test, labels_placeholder_test:y_test})\n","      \n","      #For accuracy measures, we want to use the clean encoder and the running average mean and std of the labeled points. We thus have to set num_labeled to the\n","      #full test/train size to avoid splitting. Note that we are NOT training in this step, since we are not requesting the 'train_step' operation.\n","#      if i % 100 == 0: #Remove this for more fine grained analysis.\n","#        print(\"Test accuracy is: %s\" % test_acc())\n","#         summary = sess.run(merged2, feed_dict={isTrain: False, num_labeled_tf:X_train.shape[0], features_placeholder_test: X_train, labels_placeholder_test:y_train})\n","#         train_writer.add_summary(summary, i + epoch*n_iter)\n","#         train_writer.flush()\n","      \n","#       if i % 100 == 0:\n","#         summary = sess.run(merged2, feed_dict={isTrain: False, num_labeled_tf:X_test.shape[0], features_placeholder_test: X_test, labels_placeholder_test:y_test})\n","#         test_writer.add_summary(summary, i + epoch*n_iter)\n","#         test_writer.flush()\n","        \n","    print(\"Epoch: %s Train accuracy: %s Test Accuracy: %s\" % (epoch, train_acc(),test_acc()))\n","    saver.save(sess, 'checkpoints/model.ckpt', epoch)\n","\n","  print(\"Final test accuracy is: %s\" % test_acc())\n","  sess.close()\n","    \n","  tf.reset_default_graph()\n","  return\n","        \n","\n","def delete_checkpoints():\n","  import shutil\n","  import os\n","  if os.path.exists('checkpoints/') and os.path.isdir('checkpoints/'):\n","      shutil.rmtree('checkpoints/')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-cZhR5wFKW8q","colab_type":"code","colab":{}},"cell_type":"code","source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TN7XceMjKbph","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"30eac17d-6174-428e-c0d7-bb034167eafc","executionInfo":{"status":"ok","timestamp":1541367234031,"user_tz":300,"elapsed":4021,"user":{"displayName":"julian buchel","photoUrl":"","userId":"03709653353653158720"}}},"cell_type":"code","source":["LOG_DIR = './log_ladder'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","get_ipython().system_raw('./ngrok http 6006 &')\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":32,"outputs":[{"output_type":"stream","text":["http://09bb5076.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"O5AW9ymQ4_GV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1411},"outputId":"1183d86a-c593-4051-8bdb-a8fb06014dad","executionInfo":{"status":"ok","timestamp":1541685043683,"user_tz":300,"elapsed":1845514,"user":{"displayName":"julian buchel","photoUrl":"","userId":"03709653353653158720"}}},"cell_type":"code","source":["noise_sds = [0.2,0.3,0.4]\n","for ns in noise_sds:\n","  #Shuffle the data\n","  indices = np.arange(X_train.shape[0])\n","  np.random.shuffle(indices)\n","  X_train = X_train[indices,:,:,:]\n","  y_train = y_train[indices]\n","  \n","  delete_checkpoints()\n","  train(X_train,y_train,X_test,y_test,num_epochs=10,noise_std=ns,lr=0.01,filter_size=[90,30,15],fc=[20],\n","        denoising_cost=[0.1,0.1,0.1,0.1,0.1,0.1],num_labeled=90,batch_size=100)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(128051,)\n","Size unlab batch: 110\n","(<tf.Tensor 'concat_3:0' shape=(200, 5, 5, 30) dtype=float64>, <tf.Tensor 'concat_4:0' shape=(200,) dtype=float64>)\n","[9]\n","[20]\n","relu\n","Tensor(\"flatten/Reshape:0\", shape=(?, 20), dtype=float32)\n","<tf.Variable 'Variable_19:0' shape=(20, 375) dtype=float32_ref>\n","[5, 5, 15]\n","conv\n","[5, 5, 30]\n","conv\n","[5, 5, 90]\n","conv\n","[5, 5, 30]\n","No checkpoint, initialize variables...\n","[]\n","Epoch: 0 Train accuracy: 80.21648 Test Accuracy: 69.79614\n","Epoch: 1 Train accuracy: 86.724815 Test Accuracy: 78.67028\n","Epoch: 2 Train accuracy: 88.525665 Test Accuracy: 80.92388\n","Epoch: 3 Train accuracy: 89.96728 Test Accuracy: 90.70507\n","Epoch: 4 Train accuracy: 91.75797 Test Accuracy: 92.902565\n","Epoch: 5 Train accuracy: 87.701775 Test Accuracy: 78.89471\n","Epoch: 6 Train accuracy: 88.05007 Test Accuracy: 81.00804\n","Epoch: 7 Train accuracy: 88.46007 Test Accuracy: 81.43819\n","Epoch: 8 Train accuracy: 86.68421 Test Accuracy: 77.64166\n","Epoch: 9 Train accuracy: 86.20862 Test Accuracy: 81.59715\n","Final test accuracy is: 81.59715\n","Size unlab batch: 110\n","(<tf.Tensor 'concat_3:0' shape=(200, 5, 5, 30) dtype=float64>, <tf.Tensor 'concat_4:0' shape=(200,) dtype=float64>)\n","[9]\n","[20]\n","relu\n","Tensor(\"flatten/Reshape:0\", shape=(?, 20), dtype=float32)\n","<tf.Variable 'Variable_19:0' shape=(20, 375) dtype=float32_ref>\n","[5, 5, 15]\n","conv\n","[5, 5, 30]\n","conv\n","[5, 5, 90]\n","conv\n","[5, 5, 30]\n","No checkpoint, initialize variables...\n","[]\n","Epoch: 0 Train accuracy: 82.75453 Test Accuracy: 77.061905\n","Epoch: 1 Train accuracy: 88.64749 Test Accuracy: 86.75893\n","Epoch: 2 Train accuracy: 88.28513 Test Accuracy: 88.610435\n","Epoch: 3 Train accuracy: 84.43354 Test Accuracy: 85.38433\n","Epoch: 4 Train accuracy: 88.87553 Test Accuracy: 85.50589\n","Epoch: 5 Train accuracy: 92.470184 Test Accuracy: 89.40527\n","Epoch: 6 Train accuracy: 89.82593 Test Accuracy: 86.0389\n","Epoch: 7 Train accuracy: 92.50689 Test Accuracy: 91.03236\n","Epoch: 8 Train accuracy: 88.490524 Test Accuracy: 83.26164\n","Epoch: 9 Train accuracy: 91.57054 Test Accuracy: 88.423416\n","Final test accuracy is: 88.423416\n","Size unlab batch: 110\n","(<tf.Tensor 'concat_3:0' shape=(200, 5, 5, 30) dtype=float64>, <tf.Tensor 'concat_4:0' shape=(200,) dtype=float64>)\n","[9]\n","[20]\n","relu\n","Tensor(\"flatten/Reshape:0\", shape=(?, 20), dtype=float32)\n","<tf.Variable 'Variable_19:0' shape=(20, 375) dtype=float32_ref>\n","[5, 5, 15]\n","conv\n","[5, 5, 30]\n","conv\n","[5, 5, 90]\n","conv\n","[5, 5, 30]\n","No checkpoint, initialize variables...\n","[]\n","Epoch: 0 Train accuracy: 81.47613 Test Accuracy: 73.72359\n","Epoch: 1 Train accuracy: 80.717834 Test Accuracy: 74.81766\n","Epoch: 2 Train accuracy: 85.67758 Test Accuracy: 79.28745\n","Epoch: 3 Train accuracy: 82.63192 Test Accuracy: 77.76323\n","Epoch: 4 Train accuracy: 84.12898 Test Accuracy: 80.662056\n","Epoch: 5 Train accuracy: 90.19687 Test Accuracy: 85.67421\n","Epoch: 6 Train accuracy: 90.25779 Test Accuracy: 85.16926\n","Epoch: 7 Train accuracy: 90.7365 Test Accuracy: 87.96522\n","Epoch: 8 Train accuracy: 90.59046 Test Accuracy: 85.87058\n","Epoch: 9 Train accuracy: 89.935265 Test Accuracy: 85.86123\n","Final test accuracy is: 85.86123\n"],"name":"stdout"}]},{"metadata":{"id":"ibzChSEX2-0U","colab_type":"code","colab":{}},"cell_type":"code","source":["import shutil\n","import os\n","if os.path.exists('log_ladder/') and os.path.isdir('log_ladder/'):\n","    shutil.rmtree('log_ladder/')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-v8IcUoQc2v-","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def unpool_2d(pool, \n","              ind, \n","              stride=[1, 2, 2, 1], \n","              scope='unpool_2d'):\n","  \"\"\"Adds a 2D unpooling op.\n","  https://arxiv.org/abs/1505.04366\n","  Unpooling layer after max_pool_with_argmax.\n","       Args:\n","           pool:        max pooled output tensor\n","           ind:         argmax indices\n","           stride:      stride is the same as for the pool\n","       Return:\n","           unpool:    unpooling tensor\n","  \"\"\"\n","  with tf.variable_scope(scope):\n","    input_shape = tf.shape(pool)\n","    output_shape = [input_shape[0], input_shape[1] * stride[1], input_shape[2] * stride[2], input_shape[3]]\n","\n","    flat_input_size = tf.reduce_prod(input_shape)\n","    flat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n","\n","    pool_ = tf.reshape(pool, [flat_input_size])\n","    batch_range = tf.reshape(tf.range(tf.cast(output_shape[0], tf.int64), dtype=ind.dtype), \n","                                      shape=[input_shape[0], 1, 1, 1])\n","    b = tf.ones_like(ind) * batch_range\n","    b1 = tf.reshape(b, [flat_input_size, 1])\n","    ind_ = tf.reshape(ind, [flat_input_size, 1])\n","    ind_ = tf.concat([b1, ind_], 1)\n","\n","    ret = tf.scatter_nd(ind_, pool_, shape=tf.cast(flat_output_shape, tf.int64))\n","    ret = tf.reshape(ret, output_shape)\n","\n","    set_input_shape = pool.get_shape()\n","    set_output_shape = [set_input_shape[0], set_input_shape[1] * stride[1], set_input_shape[2] * stride[2], set_input_shape[3]]\n","    ret.set_shape(set_output_shape)\n","    return ret\n","\n","input = tf.placeholder(dtype = X_test.dtype, shape=X_test.shape)\n","\n","z = tf.layers.conv2d(input,10,[5,5],padding='same') #filter_size, kernel_size\n","shape_z = tf.shape(z)\n","#Do max_pooling\n","output, argmax = tf.nn.max_pool_with_argmax(z,ksize=[1,4,4,1],strides=[1,1,1,1],padding='VALID')\n","shape_pooled = tf.shape(output)\n","\n","unpooled = unpool_2d(output,argmax,stride=[1,1,1,1])\n","shape_unpooled = tf.shape(unpooled)\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","sz,su,sp = sess.run([shape_z, shape_unpooled,shape_pooled],feed_dict={input:X_test})\n","print(sz)\n","print(su)\n","print(sp)"],"execution_count":0,"outputs":[]}]}